{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "#%matplotlib inline\n",
    "import argparse\n",
    "import gc\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "manualSeed = 999\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "\n",
    "# Root directory for dataset\n",
    "dataroot = \"/media/fico/Data/Celeba/CelebAMask-HQ\"\n",
    "\n",
    "# Number of workers for dataloader\n",
    "workers = 2\n",
    "\n",
    "# Batch size during training\n",
    "\n",
    "\n",
    "\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "nz = 512\n",
    "\n",
    "# Size of feature maps in generator\n",
    "ngf = 4\n",
    "\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr = 0.0002\n",
    "\n",
    "# Beta1 hyperparam for Adam optimizers\n",
    "beta1 = 0\n",
    "\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = 1\n",
    "\n",
    "lambda_gp = 10\n",
    "\n",
    "d_ratio = 1\n",
    "\n",
    "img_batch_size = [(4,16),(8,16),(16,16),(32,16),(64,16)]#,(128,16)]\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loaders = []\n",
    "for img_size, batch_size in img_batch_size:\n",
    "    dataset = dset.ImageFolder(root=dataroot,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(img_size),\n",
    "                               transforms.CenterCrop(img_size),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "\n",
    "    dataload = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                             shuffle=True, num_workers=workers, drop_last=True)\n",
    "    data_loaders.append(dataload)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/hukkelas/progan-pytorch/blob/master/src/models/custom_layers.py\n",
    "class PixelNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PixelNorm, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        div = torch.square(x)\n",
    "        div = torch.mean(div, dim = 1, keepdim = True)\n",
    "        div = div + 10**(-8)\n",
    "        div = torch.square(div)\n",
    "        return x/div\n",
    "\n",
    "class MiniBatchSTD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MiniBatchSTD, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        s = x.shape\n",
    "        std = x.to(torch.float32)   \n",
    "        std = std - torch.mean(std, dim=0, keepdim= True)\n",
    "        std = torch.mean(torch.square(std), dim=0)\n",
    "        std = torch.sqrt(std + 10**(-8))\n",
    "        std = torch.mean(std)\n",
    "        std = std.to(x.dtype)\n",
    "        std = std.repeat([s[0], 1, s[2], s[3]])\n",
    "        std = torch.cat([x, std], 1)\n",
    "#         print(std.shape)\n",
    "        return std\n",
    "# https://github.com/akanimax/pro_gan_pytorch/blob/master/pro_gan_pytorch/CustomLayers.py\n",
    "class conv2d_e(nn.Module):\n",
    "    def __init__(self, input_c, output_c, kernel, stride, pad):\n",
    "        super(conv2d_e, self).__init__()\n",
    "        self.weight = torch.nn.Parameter(torch.nn.init.normal_(torch.empty(output_c, input_c, kernel, kernel)))\n",
    "        self.bias = torch.nn.Parameter(torch.FloatTensor(output_c).fill_(0))\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        fan_in = (kernel*kernel) * input_c\n",
    "        self.scale = np.sqrt(2) / np.sqrt(fan_in)\n",
    "#         print(self.weight.shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.functional.conv2d(input = x, \n",
    "                         weight = self.weight * self.scale, \n",
    "                         stride = self.stride, \n",
    "                         bias = self.bias, \n",
    "                         padding = self.pad)\n",
    "        \n",
    "\n",
    "class linear_e(nn.Module):\n",
    "    def __init__(self, input_c, output_c):\n",
    "        super(linear_e, self).__init__()\n",
    "        self.weight = torch.nn.Parameter(torch.nn.init.normal_(torch.empty(output_c, input_c)))\n",
    "        self.bias = torch.nn.Parameter(torch.FloatTensor(output_c).fill_(0))\n",
    "        fan_in = input_c\n",
    "        self.scale = np.sqrt(2) / np.sqrt(fan_in)\n",
    "#         print(self.weight.shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.functional.linear(input = x, \n",
    "                         weight = self.weight * self.scale, \n",
    "                         bias = self.bias)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Gen Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Generator Code\n",
    "# Use upsample from latent or use dense layer to upsample?\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "#         self.added = nn.ModuleList([])\n",
    "        self.up_samp = nn.Upsample(scale_factor = 2)\n",
    "#         4\n",
    "        self.start = linear_e(512, 8192)\n",
    "        self.block = nn.Sequential(\n",
    "#             nn.Upsample(scale_factor = 4),\n",
    "            conv2d_e(nz, 512, 3, 1, 1),\n",
    "            nn.LeakyReLU(.2),\n",
    "            PixelNorm(),\n",
    "            conv2d_e(512, 512, 3, 1, 1),\n",
    "            nn.LeakyReLU(.2),\n",
    "            PixelNorm(),)\n",
    "        self.end = nn.Conv2d(512, 3, 1, 1, 0)\n",
    "        \n",
    "#         8\n",
    "        self.block1 = nn.Sequential(\n",
    "            self.up_samp,\n",
    "            conv2d_e(512, 512, 3, 1, 1),\n",
    "            nn.LeakyReLU(.2),\n",
    "            PixelNorm(),\n",
    "            conv2d_e(512, 512, 3, 1, 1),\n",
    "            nn.LeakyReLU(.2),\n",
    "            PixelNorm(),) \n",
    "        self.end1 = nn.Conv2d(512, 3, 1, 1, 0)\n",
    "        \n",
    "#         16\n",
    "        self.block2 = nn.Sequential(\n",
    "            self.up_samp,\n",
    "            conv2d_e(512, 512, 3, 1, 1),\n",
    "            nn.LeakyReLU(.2),\n",
    "            PixelNorm(),\n",
    "            conv2d_e(512, 512, 3, 1, 1),\n",
    "            nn.LeakyReLU(.2),\n",
    "            PixelNorm(),)\n",
    "        self.end2 = nn.Conv2d(512, 3, 1, 1, 0)\n",
    "        \n",
    "#         32\n",
    "        self.block3 = nn.Sequential(\n",
    "            self.up_samp,\n",
    "            conv2d_e(512, 512, 3, 1, 1),\n",
    "            nn.LeakyReLU(.2),\n",
    "            PixelNorm(),\n",
    "            conv2d_e(512, 512, 3, 1, 1),\n",
    "            nn.LeakyReLU(.2),\n",
    "            PixelNorm(),)\n",
    "        self.end3 = nn.Conv2d(512, 3, 1, 1, 0)\n",
    "        \n",
    "#         64\n",
    "        self.block4 = nn.Sequential(\n",
    "            self.up_samp,\n",
    "            conv2d_e(512, 256, 3, 1, 1),\n",
    "            nn.LeakyReLU(.2),\n",
    "            PixelNorm(),\n",
    "            conv2d_e(256, 256, 3, 1, 1),\n",
    "            nn.LeakyReLU(.2),\n",
    "            PixelNorm(),)\n",
    "        self.end4 = nn.Conv2d(256, 3, 1, 1, 0)\n",
    "        \n",
    "#          128\n",
    "        self.block5 = nn.Sequential(\n",
    "            self.up_samp,\n",
    "            conv2d_e(256, 128, 3, 1, 1),\n",
    "            nn.LeakyReLU(.2),\n",
    "            PixelNorm(),\n",
    "            conv2d_e(128, 128, 3, 1, 1),\n",
    "            nn.LeakyReLU(.2),\n",
    "            PixelNorm(),)\n",
    "        self.end5 = conv2d_e(128, 3, 1, 1, 0)\n",
    "\n",
    "\n",
    "#     Get this down to one if statement logic for fade in\n",
    "    def forward(self, input, res, alpha):\n",
    "#         print(input.shape)\n",
    "#         intput1 = se\n",
    "        input1 = self.start(input)\n",
    "        input1 = input1.view(-1,512,4,4)\n",
    "\n",
    "        \n",
    "        if res == 4:\n",
    "            output = self.block(input1)\n",
    "            output = self.end(output)\n",
    "        elif res == 8:\n",
    "            output = self.block(input1)\n",
    "            if alpha >= 0:\n",
    "                output_old = self.up_samp(output)\n",
    "                output_old = self.end(output_old)\n",
    "            output = self.block1(output)\n",
    "            output = self.end1(output)\n",
    "            if alpha >= 0:\n",
    "                output = alpha*output + (1-alpha)*output_old\n",
    "        elif res == 16:\n",
    "            output = self.block(input1)\n",
    "            output = self.block1(output)\n",
    "            if alpha >= 0:\n",
    "                output_old = self.up_samp(output)\n",
    "                output_old = self.end1(output_old)\n",
    "            output = self.block2(output)\n",
    "            output = self.end2(output)\n",
    "            if alpha >= 0:\n",
    "                output = alpha*output + (1-alpha)*output_old\n",
    "        elif res == 32:\n",
    "            output = self.block(input1)\n",
    "            output = self.block1(output)\n",
    "            output = self.block2(output)\n",
    "            if alpha >= 0:\n",
    "                output_old = self.up_samp(output)\n",
    "                output_old = self.end2(output_old)\n",
    "            output = self.block3(output)\n",
    "            output = self.end3(output)\n",
    "            if alpha >= 0:\n",
    "                output = alpha*output + (1-alpha)*output_old\n",
    "        elif res == 64:\n",
    "            output = self.block(input1)\n",
    "            output = self.block1(output)\n",
    "            output = self.block2(output)\n",
    "            output = self.block3(output)\n",
    "            if alpha >= 0:\n",
    "                output_old = self.up_samp(output)\n",
    "                output_old = self.end3(output_old)\n",
    "            output = self.block4(output)\n",
    "            output = self.end4(output)\n",
    "            if alpha >= 0:\n",
    "                output = alpha*output + (1-alpha)*output_old\n",
    "        elif res == 128:\n",
    "            output = self.block(input1)\n",
    "            output = self.block1(output)\n",
    "            output = self.block2(output)\n",
    "            output = self.block3(output)\n",
    "            output = self.block4(output)\n",
    "            if alpha >= 0:\n",
    "                output_old = self.up_samp(output)\n",
    "                output_old = self.end4(output_old)\n",
    "            output = self.block5(output)\n",
    "            output = self.end5(output)\n",
    "            if alpha >= 0:\n",
    "                output = alpha*output + (1-alpha)*output_old\n",
    "            \n",
    "#         print(output.shape) \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Dis Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Implement batchstdev\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "#         self.added = nn.ModuleList([])\n",
    "        self.down_samp = nn.AvgPool2d(2)\n",
    "    \n",
    "#         4\n",
    "        self.block = nn.Sequential(\n",
    "            MiniBatchSTD(),\n",
    "            conv2d_e(513, 512, 3, 1, 1),\n",
    "            nn.LeakyReLU(.2),\n",
    "            conv2d_e(512, 512, 4, 1, 0),\n",
    "            nn.LeakyReLU(.2),\n",
    "            nn.Flatten(),\n",
    "            linear_e(512, 1))\n",
    "        self.start = conv2d_e(3, 512, 1, 1, 0)\n",
    "        \n",
    "#         8\n",
    "        self.block1 = nn.Sequential(\n",
    "            conv2d_e(512, 512, 3, 1, 1),\n",
    "            nn.LeakyReLU(.2),\n",
    "            conv2d_e(512, 512, 3, 1, 1),\n",
    "            nn.LeakyReLU(.2),\n",
    "            self.down_samp,)\n",
    "        self.start1 = conv2d_e(3, 512, 1, 1, 0)\n",
    "\n",
    "#         16\n",
    "        self.block2 = nn.Sequential(\n",
    "            conv2d_e(512, 512, 3, 1, 1),\n",
    "            nn.LeakyReLU(.2),\n",
    "            conv2d_e(512, 512, 3, 1, 1),\n",
    "            nn.LeakyReLU(.2),\n",
    "            self.down_samp,)\n",
    "        self.start2 = conv2d_e(3, 512, 1, 1, 0)\n",
    "        \n",
    "#         32\n",
    "        self.block3 = nn.Sequential(\n",
    "            conv2d_e(512, 512, 3, 1, 1),\n",
    "            nn.LeakyReLU(.2),\n",
    "            conv2d_e(512, 512, 3, 1, 1),\n",
    "            nn.LeakyReLU(.2),\n",
    "            self.down_samp,)\n",
    "        self.start3 = conv2d_e(3, 512, 1, 1, 0)\n",
    "        \n",
    "#         64\n",
    "        self.block4 = nn.Sequential(\n",
    "            conv2d_e(256, 256, 3, 1, 1),\n",
    "            nn.LeakyReLU(.2),\n",
    "            conv2d_e(256, 512, 3, 1, 1),\n",
    "            nn.LeakyReLU(.2),\n",
    "            self.down_samp,)\n",
    "        self.start4 = conv2d_e(3, 256, 1, 1, 0)\n",
    "        \n",
    "#         128\n",
    "        self.block5= nn.Sequential(\n",
    "            conv2d_e(128, 128, 3, 1, 1),\n",
    "            nn.LeakyReLU(.2),\n",
    "            conv2d_e(128, 256, 3, 1, 1),\n",
    "            nn.LeakyReLU(.2),\n",
    "            self.down_samp,)\n",
    "        self.start5 = conv2d_e(3, 128, 1, 1, 0)\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, input, res, alpha):   \n",
    "        \n",
    "        if res == 4:\n",
    "            output = self.start(input)\n",
    "            output = self.block(output)\n",
    "            \n",
    "        elif res == 8:\n",
    "            output = self.start1(input)\n",
    "            output = self.block1(output)\n",
    "            \n",
    "            if alpha >= 0:\n",
    "                output_old = self.down_samp(input)\n",
    "                output_old = self.start(output_old)\n",
    "                output = alpha*output + (1-alpha)*output_old\n",
    "            \n",
    "            output = self.block(output)\n",
    "        \n",
    "        elif res == 16:\n",
    "            output = self.start2(input)\n",
    "            output = self.block2(output)\n",
    "            \n",
    "            if alpha >= 0:\n",
    "                output_old = self.down_samp(input)\n",
    "                output_old = self.start1(output_old)\n",
    "                output = alpha*output + (1-alpha)*output_old\n",
    "                \n",
    "            output = self.block1(output)\n",
    "            output = self.block(output)\n",
    "            \n",
    "        elif res == 32:\n",
    "            output = self.start3(input)\n",
    "            output = self.block3(output)\n",
    "            \n",
    "            if alpha >= 0:\n",
    "                output_old = self.down_samp(input)\n",
    "                output_old = self.start2(output_old)\n",
    "                output = alpha*output + (1-alpha)*output_old\n",
    "            \n",
    "            output = self.block2(output)\n",
    "            output = self.block1(output)\n",
    "            output = self.block(output)\n",
    "            \n",
    "        elif res == 64:\n",
    "            output = self.start4(input)\n",
    "            output = self.block4(output)\n",
    "            \n",
    "            if alpha >= 0:\n",
    "                output_old = self.down_samp(input)\n",
    "                output_old = self.start3(output_old)\n",
    "                output = alpha*output + (1-alpha)*output_old\n",
    "                \n",
    "            output = self.block3(output)\n",
    "            output = self.block2(output)\n",
    "            output = self.block1(output)\n",
    "            output = self.block(output)\n",
    "            \n",
    "        elif res == 128:\n",
    "            output = self.start5(input)\n",
    "            output = self.block5(output)\n",
    "            \n",
    "            if alpha >= 0:\n",
    "                output_old = self.down_samp(input)\n",
    "                output_old = self.start4(output_old)\n",
    "                output = alpha*output + (1-alpha)*output_old\n",
    "            \n",
    "            output = self.block4(output)\n",
    "            output = self.block3(output)\n",
    "            output = self.block2(output)\n",
    "            output = self.block1(output)\n",
    "            output = self.block(output)\n",
    "            \n",
    "#         print(output.shape) \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# test = torch.randn(16, nz, device=device)\n",
    "# test1 = torch.randn(16, 3, 128, 128, device=device)\n",
    "\n",
    "# temp = netG(test, res = 4, alpha = -1)\n",
    "# # print(temp.shape)\n",
    "# netD(temp, res = 4, alpha = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Res: 4 Fade_in: False Iter: 16 alpha: -1\n",
      "W with GP: -2.9924731254577637 Loss G: 19.59375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def training(r = -1, e = -1, mixed_precision = False):\n",
    "    netG = Generator(ngpu).to(device)\n",
    "    netD = Discriminator(ngpu).to(device)\n",
    "    scalerD = -1\n",
    "    ScalderG = -1\n",
    "    if mixed_precision:\n",
    "        scalerD = torch.cuda.amp.GradScaler()\n",
    "        scalerG = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    # Setup Adam optimizers for both G and D\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr=0.001, betas=(0, 0.99))\n",
    "    optimizerG = optim.Adam(netG.parameters(), lr=0.001, betas=(0, 0.99))\n",
    "    training = True\n",
    "    current_data = 0\n",
    "    res = 4\n",
    "    fade_in = False\n",
    "    epoch = 0\n",
    "    \n",
    "    if(e > -1):\n",
    "        if mixed_precision:\n",
    "            pathD = \"./Saved_Models_AMP/\" + \"D_Epoch:\" + str(e)\n",
    "            pathG = \"./Saved_Models_AMP/\" + \"G_Epoch:\" + str(e)\n",
    "        checkD = torch.load(pathD)\n",
    "        checkG = torch.load(pathG)\n",
    "        optimizerD.load_state_dict(checkD['optimizer_state_dict'])\n",
    "        netD.load_state_dict(checkD['model_state_dict'])\n",
    "        scalerD.load_state_dict(checkD['scaler_state_dict'])\n",
    "        \n",
    "        optimizerG.load_state_dict(checkG['optimizer_state_dict'])\n",
    "        netG.load_state_dict(checkG['model_state_dict'])\n",
    "        scalerG.load_state_dict(checkG['scaler_state_dict'])\n",
    "        \n",
    "        epoch = e + 1\n",
    "        res = r\n",
    "        fade_in = True\n",
    "        current_data = (res//4) - 1\n",
    "        \n",
    "    fixed_noise = torch.randn(16, nz, device=device)\n",
    "\n",
    "    \n",
    "    \n",
    "    while(training):\n",
    "        steps = 800000\n",
    "        loader = iter(data_loaders[current_data])\n",
    "        count = 0\n",
    "        \n",
    "    \n",
    "        while count < steps:\n",
    "            try:\n",
    "                img = loader.next()\n",
    "            except StopIteration:\n",
    "                loader = iter(data_loaders[current_data])\n",
    "                img = loader.next()\n",
    "            if not fade_in:\n",
    "                alpha = -1\n",
    "               \n",
    "            else:\n",
    "                alpha = count/steps\n",
    "                \n",
    "            mini_batch = len(img[0])\n",
    "    \n",
    "            real_imgs = img[0].to(device)\n",
    "        \n",
    "            netD.zero_grad()\n",
    "            \n",
    "#             Discriminator loss on real images\n",
    "            if mixed_precision:\n",
    "#                 with torch.cuda.amp.autocast():\n",
    "                output_real = netD(real_imgs, alpha=alpha, res=res).squeeze()\n",
    "            else:\n",
    "                output_real = netD(real_imgs, alpha=alpha, res=res).squeeze()\n",
    "\n",
    "            \n",
    "#             Discriminator loss on fake images\n",
    "           \n",
    "            noise = torch.randn(mini_batch, nz, device=device)\n",
    "            \n",
    "            if mixed_precision:\n",
    "#                 with torch.cuda.amp.autocast():\n",
    "                fake_imgs = netG(noise, res=res, alpha=alpha)\n",
    "                output_fake = netD(fake_imgs.detach(), alpha=alpha, res=res).squeeze()\n",
    "                    \n",
    "            else:\n",
    "                fake_imgs = netG(noise, res=res, alpha=alpha)\n",
    "                output_fake = netD(fake_imgs.detach(), alpha=alpha, res=res).squeeze()\n",
    "                \n",
    "                \n",
    "                \n",
    "#             Gradient Penalty\n",
    "            \n",
    "            gp_alpha = torch.randn(mini_batch, 1, 1, 1, device = device)\n",
    "            interp = gp_alpha * real_imgs + ((1-gp_alpha) * fake_imgs.detach())\n",
    "            interp.requires_grad = True\n",
    "            \n",
    "            if mixed_precision:\n",
    "#                 with torch.cuda.amp.autocast():\n",
    "                model_interp = netD(interp, alpha = alpha, res = res)\n",
    "            else:\n",
    "                model_interp = netD(interp, alpha = alpha, res = res)\n",
    "            \n",
    "            if mixed_precision:\n",
    "\n",
    "                grads = torch.autograd.grad(outputs=model_interp, inputs=interp,\n",
    "                          grad_outputs=torch.ones(model_interp.size()).to(device),\n",
    "                          create_graph=True, retain_graph=True, only_inputs=True, allow_unused=True)[0]\n",
    "#                 inv_scale = 1./scalerD.get_scale()\n",
    "#                 grads = grads* inv_scale\n",
    "#              \n",
    "#                 with torch.cuda.amp.autocast(): \n",
    "                grads = torch.square(grads)\n",
    "                grads = torch.sum(grads, dim = [1,2,3])\n",
    "                grads = torch.sqrt(grads)\n",
    "                grads = grads - 1\n",
    "                grads = torch.square(grads)\n",
    "                grad_pen = grads * lambda_gp\n",
    "\n",
    "            else:\n",
    "                \n",
    "                grads = torch.autograd.grad(outputs=model_interp, inputs=interp,\n",
    "                              grad_outputs=torch.ones(model_interp.size()).to(device),\n",
    "                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "                grads = torch.square(grads)\n",
    "                grads = torch.sum(grads, dim = [1,2,3])\n",
    "                grads = torch.sqrt(grads)\n",
    "                grads = grads - 1\n",
    "                grads = torch.square(grads)\n",
    "                grad_pen = grads * lambda_gp\n",
    "\n",
    "#             Extra small penalty\n",
    "            if mixed_precision:\n",
    "#                 with torch.cuda.amp.autocast():\n",
    "                penalty = torch.square(output_real)\n",
    "                penalty = penalty * .001\n",
    "            else:\n",
    "                penalty = torch.square(output_real)\n",
    "                penalty = penalty * .001\n",
    "\n",
    "#             Calculating entire loss and taking step\n",
    "            \n",
    "            if mixed_precision:\n",
    "#                 with torch.cuda.amp.autocast():\n",
    "                loss_D = torch.mean(output_fake - output_real + grad_pen + penalty)\n",
    "\n",
    "                scalerD.scale(loss_D).backward()\n",
    "\n",
    "\n",
    "                scalerD.step(optimizerD)\n",
    "                scalerD.update()\n",
    "\n",
    "    \n",
    "            else:\n",
    "                loss_D = torch.mean(output_fake - output_real + grad_pen + penalty)\n",
    "\n",
    "\n",
    "                loss_D.backward()\n",
    "\n",
    "                optimizerD.step()\n",
    "    #             loss_D = loss_fake + loss_real + grad_pen\n",
    "            \n",
    "                \n",
    "                \n",
    "            netG.zero_grad()\n",
    "            \n",
    "#             Generator loss on created batch\n",
    "            if mixed_precision:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    output = netD(fake_imgs, alpha=alpha, res=res).squeeze()\n",
    "                    loss_G = -torch.mean(output)\n",
    "                scalerG.scale(loss_G).backward() \n",
    "                scalerG.step(optimizerD)\n",
    "                scalerG.update()\n",
    "            else:\n",
    "                output = netD(fake_imgs, alpha=alpha, res=res).squeeze()\n",
    "                loss_G = -torch.mean(output)\n",
    "                loss_G.backward()\n",
    "\n",
    "                optimizerG.step()\n",
    "\n",
    "#             Training Stats\n",
    "                \n",
    "            count += mini_batch\n",
    "            if count %5000 <= mini_batch:\n",
    "                print(\"Res:\", res, \"Fade_in:\", fade_in, \"Iter:\",count, \"alpha:\", alpha)\n",
    "                print(\"W with GP:\", loss_D.item(),  \"Loss G:\", loss_G.item())\n",
    "#                 print(\"Loss real:\", loss_real.item(), \"Loss Fake:\", loss_fake.item())\n",
    "                print()\n",
    "                with torch.no_grad():\n",
    "                    guess = netG(fixed_noise, res = res, alpha=alpha)\n",
    "                    guess = guess.cpu()\n",
    "                #     guess = next(iter(data_loaders[0]))[0]\n",
    "                #     print(guess[0].shape)\n",
    "                    old_min = torch.min(guess)\n",
    "                    old_max = torch.max(guess)\n",
    "                    old_range = old_max - old_min\n",
    "                    new_range = 1 - 0\n",
    "                    guess = (((guess - old_min)*new_range)/ old_range) + 0\n",
    "                #     guess = guess.float()\n",
    "#                     print(guess.shape, torch.min(guess), torch.max(guess))\n",
    "                #     guess = ((guess *.5 ) + .5)\n",
    "                    guess = guess.permute(0,2,3,1)\n",
    "#                     print(guess.shape, torch.min(guess[0]), torch.max(guess[0]))\n",
    "\n",
    "\n",
    "                    fig = plt.figure(figsize=(4,4))\n",
    "                    for i in range(16):\n",
    "                        plt.subplot(4, 4, i+1)\n",
    "                        plt.imshow(guess[i, :, :])\n",
    "                        plt.axis('off')\n",
    "                    if mixed_precision:\n",
    "                        path = \"./Training_Imgs_AMP/Epoch: \" + str(epoch) + \" training_step: \" + str(count) + \".png\"\n",
    "                    else:\n",
    "                        path = \"./Training_Imgs_AMP/Epoch: \" + str(epoch) + \" training_step: \" + str(count) + \".png\"\n",
    "                    plt.savefig(path, dpi=300)\n",
    "                    plt.close('all')\n",
    "        \n",
    "        if mixed_precision:\n",
    "            save_path_D = \"./Saved_Models_AMP/\" + \"D_Epoch:\" + str(epoch)\n",
    "            save_path_G = \"./Saved_Models_AMP/\" + \"G_Epoch:\" + str(epoch)\n",
    "        else:\n",
    "            save_path_D = \"./Saved_Models/\" + \"D_Epoch:\" + str(epoch)\n",
    "            save_path_G = \"./Saved_Models/\" + \"G_Epoch:\" + str(epoch)\n",
    "         \n",
    "            \n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': netD.state_dict(),\n",
    "            'optimizer_state_dict': optimizerD.state_dict(),\n",
    "            'loss': loss_D,\n",
    "            'scaler_state_dict': scalerD.state_dict()\n",
    "            \n",
    "            }, save_path_D)\n",
    "        \n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': netG.state_dict(),\n",
    "            'optimizer_state_dict': optimizerG.state_dict(),\n",
    "            'loss': loss_G,\n",
    "            'fixednoise': fixed_noise,\n",
    "            'scaler_state_dict': scalerG.state_dict()\n",
    "\n",
    "            }, save_path_G)\n",
    "        \n",
    "        if fade_in == False:\n",
    "            fade_in = True\n",
    "            current_data += 1\n",
    "            if current_data == len(data_loaders):\n",
    "                training= False\n",
    "            res = res * 2\n",
    "        else:\n",
    "            fade_in = False\n",
    "        epoch += 1\n",
    "    \n",
    "    \n",
    "    \n",
    "training(r=-1, e=-1, mixed_precision = True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# OldRange = (OldMax - OldMin)  \n",
    "# NewRange = (NewMax - NewMin)  \n",
    "# NewValue = (((OldValue - OldMin) * NewRange) / OldRange) + NewMin\n",
    "\n",
    "\n",
    "fixed_noise = torch.randn(256, nz, device=device)\n",
    "with torch.no_grad():\n",
    "    guess = netG(fixed_noise, res = 4, alpha=-1)\n",
    "    guess = guess.cpu()\n",
    "#     guess = next(iter(data_loaders[0]))[0]\n",
    "#     print(guess[0].shape)\n",
    "    old_min = torch.min(guess)\n",
    "    old_max = torch.max(guess)\n",
    "    old_range = old_max - old_min\n",
    "    new_range = 1 - 0\n",
    "    guess = (((guess - old_min)*new_range)/ old_range) + 0\n",
    "#     guess = guess.float()\n",
    "    print(guess.shape, torch.min(guess), torch.max(guess))\n",
    "#     guess = ((guess *.5 ) + .5)\n",
    "    guess = guess.permute(0,2,3,1)\n",
    "    print(guess.shape, torch.min(guess[0]), torch.max(guess[0]))\n",
    "\n",
    "\n",
    "    fig = plt.figure(figsize=(4,4))\n",
    "    for i in range(16):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        plt.imshow(guess[i, :, :])\n",
    "        plt.axis('off')\n",
    "#     plt.savefig('end_train.png', dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First get it working without mixed precision  \n",
    "\n",
    "\n",
    "\n",
    "# #             Discriminator loss on real images\n",
    "#             with torch.cuda.amp.autocast(): \n",
    "#                 output_real = netD(real_imgs, alpha=alpha, res=res).squeeze()\n",
    "#                 loss_real = -torch.mean(output_real)\n",
    "# #                 print(loss_real)\n",
    "# #             print(loss_real) \n",
    "#             scalerD.scale(loss_real).backward()\n",
    "# #             print(loss_real)\n",
    "\n",
    "            \n",
    "#             noise = torch.randn(mini_batch, nz, 1, 1, device=device)\n",
    "            \n",
    "# #             Discriminator loss on fake images\n",
    "#             with torch.cuda.amp.autocast():\n",
    "            \n",
    "#                 fake_imgs = netG(noise, res=res, alpha=alpha)\n",
    "#                 output_fake = netD(fake_imgs.detach(), alpha=alpha, res=res).squeeze()\n",
    "                \n",
    "#                 loss_fake = torch.mean(output_fake)\n",
    "            \n",
    "#             scalerD.scale(loss_fake).backward()\n",
    "            \n",
    "# #             Gradient Penalty\n",
    "#             with torch.cuda.amp.autocast():\n",
    "#                 gp_alpha = torch.randn(mini_batch, 1, 1, 1, device = device)\n",
    "#                 gp_alpha = gp_alpha.expand(real_imgs.size(0), real_imgs.size(1), real_imgs.size(2), real_imgs.size(3))\n",
    "#                 interp = gp_alpha * real_imgs + ((1-gp_alpha) * fake_imgs.detach())\n",
    "#                 interp.requires_grad = True\n",
    "#                 model_interp = netD(interp, alpha = alpha, res = res)\n",
    "            \n",
    "#             grads = torch.autograd.grad(outputs=model_interp, inputs=interp,\n",
    "#                               grad_outputs=torch.ones(model_interp.size()).to(device),\n",
    "#                               create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "            \n",
    "# #             Do I need to unscale the gradients for calculating gp for wasserstein?\n",
    "# #             print(scaled_grad_params.shape)\n",
    "# #             inv_scale = 1./scalerD.get_scale()\n",
    "# #             grads = [p * inv_scale for p in scaled_grad_params]\n",
    "# #             print(grads)\n",
    "            \n",
    "#             with torch.cuda.amp.autocast():\n",
    "#                 grads = ((grads.norm(2, dim=1) - 1) ** 2).mean()\n",
    "#                 grads += 1e-16\n",
    "#                 grad_pen = grads * lambda_gp\n",
    "# #                 print(grad_pen.item())\n",
    "            \n",
    "#             scalerD.scale(grad_pen).backward()\n",
    "# #             print(grad_pen.item())\n",
    "# #             return\n",
    "#             scalerD.step(optimizerD)\n",
    "#             scalerD.update()\n",
    "#             loss_D = loss_fake + loss_real + grad_pen\n",
    "            \n",
    "                \n",
    "                \n",
    "#             netG.zero_grad()\n",
    "            \n",
    "\n",
    "#             with torch.cuda.amp.autocast():\n",
    "#                 output = netD(fake_imgs, alpha=alpha, res=res).squeeze()\n",
    "#                 loss_G = -torch.mean(output)\n",
    "#             scalerG.scale(loss_G).backward()    \n",
    "\n",
    "#             scalerG.step(optimizerG)\n",
    "#             scalerG.update()\n",
    "                \n",
    "            \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
