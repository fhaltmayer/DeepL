{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  999\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "#%matplotlib inline\n",
    "import argparse\n",
    "import gc\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "manualSeed = 999\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "\n",
    "# Root directory for dataset\n",
    "dataroot = \"/media/fico/Data/Celeba/CelebAMask-HQ\"\n",
    "\n",
    "# Number of workers for dataloader\n",
    "workers = 2\n",
    "\n",
    "# Batch size during training\n",
    "\n",
    "\n",
    "\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "nz = 512\n",
    "\n",
    "# Size of feature maps in generator\n",
    "ngf = 4\n",
    "\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr = 0.0002\n",
    "\n",
    "# Beta1 hyperparam for Adam optimizers\n",
    "beta1 = 0\n",
    "\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = 1\n",
    "\n",
    "lambda_gp = 10\n",
    "\n",
    "d_ratio = 1\n",
    "\n",
    "img_batch_size = [(4,16),(8,16),(16,16),(32,16),(64,16),(128,16)]\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.8980) tensor(0.8510)\n"
     ]
    }
   ],
   "source": [
    "data_loaders = []\n",
    "for img_size, batch_size in img_batch_size:\n",
    "    dataset = dset.ImageFolder(root=dataroot,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(img_size),\n",
    "                               transforms.CenterCrop(img_size),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "\n",
    "    dataload = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                             shuffle=True, num_workers=workers, drop_last=True)\n",
    "    data_loaders.append(dataload)\n",
    "\n",
    "x = next(iter(data_loaders[0]))\n",
    "print(torch.min(x[0]), torch.max(x[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/hukkelas/progan-pytorch/blob/master/src/models/custom_layers.py\n",
    "class PixelNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        div = torch.square(x)\n",
    "        div = torch.mean(div, dim = 1, keepdim = True)\n",
    "        div = div + 10**(-8)\n",
    "        div = torch.square(div)\n",
    "        return x/div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator Code\n",
    "# Use upsample from latent or use dense layer to upsample?\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "#         self.added = nn.ModuleList([])\n",
    "        self.up_samp = nn.Upsample(scale_factor = 2)\n",
    "#         4\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Upsample(scale_factor = 4),\n",
    "            nn.Conv2d(nz, 512, 3, 1, 1),\n",
    "            nn.LeakyReLU(True),\n",
    "            PixelNorm(),\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.LeakyReLU(True),\n",
    "            PixelNorm(),)\n",
    "        self.end = nn.Conv2d(512, 3, 1, 1, 0)\n",
    "        \n",
    "#         8\n",
    "        self.block1 = nn.Sequential(\n",
    "            self.up_samp,\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.LeakyReLU(True), \n",
    "            PixelNorm(),\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.LeakyReLU(True), \n",
    "            PixelNorm(),) \n",
    "        self.end1 = nn.Conv2d(512, 3, 1, 1, 0)\n",
    "        \n",
    "#         16\n",
    "        self.block2 = nn.Sequential(\n",
    "            self.up_samp,\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.LeakyReLU(True),\n",
    "            PixelNorm(),\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.LeakyReLU(True),\n",
    "            PixelNorm(),)\n",
    "        self.end2 = nn.Conv2d(512, 3, 1, 1, 0)\n",
    "        \n",
    "#         32\n",
    "        self.block3 = nn.Sequential(\n",
    "            self.up_samp,\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.LeakyReLU(True),\n",
    "            PixelNorm(),\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.LeakyReLU(True),\n",
    "            PixelNorm(),)\n",
    "        self.end3 = nn.Conv2d(512, 3, 1, 1, 0)\n",
    "        \n",
    "#         64\n",
    "        self.block4 = nn.Sequential(\n",
    "            self.up_samp,\n",
    "            nn.Conv2d(512, 256, 3, 1, 1),\n",
    "            nn.LeakyReLU(True),\n",
    "            PixelNorm(),\n",
    "            nn.Conv2d(256, 256, 3, 1, 1),\n",
    "            nn.LeakyReLU(True),\n",
    "            PixelNorm(),)\n",
    "        self.end4 = nn.Conv2d(256, 3, 1, 1, 0)\n",
    "        \n",
    "#          128\n",
    "        self.block5 = nn.Sequential(\n",
    "            self.up_samp,\n",
    "            nn.Conv2d(256, 128, 3, 1, 1),\n",
    "            nn.LeakyReLU(True),\n",
    "            PixelNorm(),\n",
    "            nn.Conv2d(128, 128, 3, 1, 1),\n",
    "            nn.LeakyReLU(True),\n",
    "            PixelNorm(),)\n",
    "        self.end5 = nn.Conv2d(128, 3, 1, 1, 0)\n",
    "\n",
    "\n",
    "#     Get this down to one if statement logic for fade in\n",
    "    def forward(self, input, res, alpha):\n",
    "        \n",
    "        \n",
    "        \n",
    "        if res == 4:\n",
    "            output = self.block(input)\n",
    "            output = self.end(output)\n",
    "        elif res == 8:\n",
    "            output = self.block(input)\n",
    "            if alpha >= 0:\n",
    "                output_old = self.up_samp(output)\n",
    "                output_old = self.end(output_old)\n",
    "            output = self.block1(output)\n",
    "            output = self.end1(output)\n",
    "            if alpha >= 0:\n",
    "                output = alpha*output + (1-alpha)*output_old\n",
    "        elif res == 16:\n",
    "            output = self.block(input)\n",
    "            output = self.block1(output)\n",
    "            if alpha >= 0:\n",
    "                output_old = self.up_samp(output)\n",
    "                output_old = self.end1(output_old)\n",
    "            output = self.block2(output)\n",
    "            output = self.end2(output)\n",
    "            if alpha >= 0:\n",
    "                output = alpha*output + (1-alpha)*output_old\n",
    "        elif res == 32:\n",
    "            output = self.block(input)\n",
    "            output = self.block1(output)\n",
    "            output = self.block2(output)\n",
    "            if alpha >= 0:\n",
    "                output_old = self.up_samp(output)\n",
    "                output_old = self.end2(output_old)\n",
    "            output = self.block3(output)\n",
    "            output = self.end3(output)\n",
    "            if alpha >= 0:\n",
    "                output = alpha*output + (1-alpha)*output_old\n",
    "        elif res == 64:\n",
    "            output = self.block(input)\n",
    "            output = self.block1(output)\n",
    "            output = self.block2(output)\n",
    "            output = self.block3(output)\n",
    "            if alpha >= 0:\n",
    "                output_old = self.up_samp(output)\n",
    "                output_old = self.end3(output_old)\n",
    "            output = self.block4(output)\n",
    "            output = self.end4(output)\n",
    "            if alpha >= 0:\n",
    "                output = alpha*output + (1-alpha)*output_old\n",
    "        elif res == 128:\n",
    "            output = self.block(input)\n",
    "            output = self.block1(output)\n",
    "            output = self.block2(output)\n",
    "            output = self.block3(output)\n",
    "            output = self.block4(output)\n",
    "            if alpha >= 0:\n",
    "                output_old = self.up_samp(output)\n",
    "                output_old = self.end4(output_old)\n",
    "            output = self.block5(output)\n",
    "            output = self.end5(output)\n",
    "            if alpha >= 0:\n",
    "                output = alpha*output + (1-alpha)*output_old\n",
    "            \n",
    "#         print(output.shape) \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement batchstdev\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "#         self.added = nn.ModuleList([])\n",
    "        self.down_samp = nn.AvgPool2d(2)\n",
    "    \n",
    "#         4\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Conv2d(512, 512, 4, 1, 0),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, 1))\n",
    "        self.start = nn.Conv2d(3, 512, 1, 1, 0)\n",
    "        \n",
    "#         8\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.LeakyReLU(True),\n",
    "            self.down_samp,)\n",
    "        self.start1 = nn.Conv2d(3, 512, 1, 1, 0)\n",
    "\n",
    "#         16\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.LeakyReLU(True),\n",
    "            self.down_samp,)\n",
    "        self.start2 = nn.Conv2d(3, 512, 1, 1, 0)\n",
    "        \n",
    "#         32\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.LeakyReLU(True),\n",
    "            self.down_samp,)\n",
    "        self.start3 = nn.Conv2d(3, 512, 1, 1, 0)\n",
    "        \n",
    "#         64\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, 3, 1, 1),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Conv2d(256, 512, 3, 1, 1),\n",
    "            nn.LeakyReLU(True),\n",
    "            self.down_samp,)\n",
    "        self.start4 = nn.Conv2d(3, 256, 1, 1, 0)\n",
    "        \n",
    "#         128\n",
    "        self.block5= nn.Sequential(\n",
    "            nn.Conv2d(128, 128, 3, 1, 1),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Conv2d(128, 256, 3, 1, 1),\n",
    "            nn.LeakyReLU(True),\n",
    "            self.down_samp,)\n",
    "        self.start5 = nn.Conv2d(3, 128, 1, 1, 0)\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, input, res, alpha):   \n",
    "        \n",
    "        if res == 4:\n",
    "            output = self.start(input)\n",
    "            output = self.block(output)\n",
    "            \n",
    "        elif res == 8:\n",
    "            output = self.start1(input)\n",
    "            output = self.block1(output)\n",
    "            \n",
    "            if alpha >= 0:\n",
    "                output_old = self.down_samp(input)\n",
    "                output_old = self.start(output_old)\n",
    "                output = alpha*output + (1-alpha)*output_old\n",
    "            \n",
    "            output = self.block(output)\n",
    "        \n",
    "        elif res == 16:\n",
    "            output = self.start2(input)\n",
    "            output = self.block2(output)\n",
    "            \n",
    "            if alpha >= 0:\n",
    "                output_old = self.down_samp(input)\n",
    "                output_old = self.start1(output_old)\n",
    "                output = alpha*output + (1-alpha)*output_old\n",
    "                \n",
    "            output = self.block1(output)\n",
    "            output = self.block(output)\n",
    "            \n",
    "        elif res == 32:\n",
    "            output = self.start3(input)\n",
    "            output = self.block3(output)\n",
    "            \n",
    "            if alpha >= 0:\n",
    "                output_old = self.down_samp(input)\n",
    "                output_old = self.start2(output_old)\n",
    "                output = alpha*output + (1-alpha)*output_old\n",
    "            \n",
    "            output = self.block2(output)\n",
    "            output = self.block1(output)\n",
    "            output = self.block(output)\n",
    "            \n",
    "        elif res == 64:\n",
    "            output = self.start4(input)\n",
    "            output = self.block4(output)\n",
    "            \n",
    "            if alpha >= 0:\n",
    "                output_old = self.down_samp(input)\n",
    "                output_old = self.start3(output_old)\n",
    "                output = alpha*output + (1-alpha)*output_old\n",
    "                \n",
    "            output = self.block3(output)\n",
    "            output = self.block2(output)\n",
    "            output = self.block1(output)\n",
    "            output = self.block(output)\n",
    "            \n",
    "        elif res == 128:\n",
    "            output = self.start5(input)\n",
    "            output = self.block5(output)\n",
    "            \n",
    "            if alpha >= 0:\n",
    "                output_old = self.down_samp(input)\n",
    "                output_old = self.start4(output_old)\n",
    "                output = alpha*output + (1-alpha)*output_old\n",
    "            \n",
    "            output = self.block4(output)\n",
    "            output = self.block3(output)\n",
    "            output = self.block2(output)\n",
    "            output = self.block1(output)\n",
    "            output = self.block(output)\n",
    "            \n",
    "#         print(output.shape) \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "netG = Generator(ngpu).to(device)\n",
    "netD = Discriminator(ngpu).to(device)\n",
    "# with torch.cuda.amp.autocast(): \n",
    "#     test = torch.randn(16, nz, 1, 1, device=device)\n",
    "#     test1 = torch.randn(16, 3, 128, 128, device=device)\n",
    "    \n",
    "#     temp = netG(test, res = 8, alpha = 1)\n",
    "#     print(temp.shape)\n",
    "#     netD(temp, res = 8, alpha = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Res: 4 Fade_in: False Iter: 16\n",
      "W with GP: -0.5160878300666809 Loss G: 2.131256341934204\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 512\n",
      "W with GP: -0.13915963470935822 Loss G: 0.29908040165901184\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 1008\n",
      "W with GP: -0.35971641540527344 Loss G: 0.7424334287643433\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 1504\n",
      "W with GP: -0.19232740998268127 Loss G: 0.560315728187561\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 2000\n",
      "W with GP: -0.6993105411529541 Loss G: 0.5690029859542847\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 2016\n",
      "W with GP: -0.7626687288284302 Loss G: 0.05118679627776146\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 2512\n",
      "W with GP: -0.26938119530677795 Loss G: 0.5851725339889526\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 3008\n",
      "W with GP: 0.1173449158668518 Loss G: 0.14312727749347687\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 3504\n",
      "W with GP: 0.3001963496208191 Loss G: -0.28552132844924927\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 4000\n",
      "W with GP: 0.08011451363563538 Loss G: -0.07044161111116409\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 4016\n",
      "W with GP: -0.599428653717041 Loss G: 0.6739310026168823\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 4512\n",
      "W with GP: -0.27215221524238586 Loss G: 0.360406756401062\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 5008\n",
      "W with GP: 1.2431137561798096 Loss G: -0.9512978792190552\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 5504\n",
      "W with GP: 0.010521620512008667 Loss G: 0.09913535416126251\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 6000\n",
      "W with GP: -0.12379661202430725 Loss G: 0.24445126950740814\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 6016\n",
      "W with GP: -0.2411973774433136 Loss G: 0.26790285110473633\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 6512\n",
      "W with GP: 0.09756758064031601 Loss G: -0.19733202457427979\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 7008\n",
      "W with GP: 1.3967642784118652 Loss G: -0.9421284794807434\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 7504\n",
      "W with GP: 0.2624228596687317 Loss G: -0.003790374845266342\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 8000\n",
      "W with GP: 0.04136340320110321 Loss G: 0.17046405375003815\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 8016\n",
      "W with GP: -0.8210678100585938 Loss G: 0.872718334197998\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 8512\n",
      "W with GP: -0.20960256457328796 Loss G: 0.15840332210063934\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 9008\n",
      "W with GP: -0.5563621520996094 Loss G: 0.2814945578575134\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 9504\n",
      "W with GP: -0.3812451958656311 Loss G: 0.11687280237674713\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 10000\n",
      "W with GP: -0.2738991677761078 Loss G: 0.4318486154079437\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 10016\n",
      "W with GP: -0.5605974197387695 Loss G: 0.3383125960826874\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 10512\n",
      "W with GP: 0.6701245307922363 Loss G: -0.5283583998680115\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 11008\n",
      "W with GP: -1.7538797855377197 Loss G: 2.541175365447998\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 11504\n",
      "W with GP: 0.29988807439804077 Loss G: 0.02974821627140045\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 12000\n",
      "W with GP: 0.10867931693792343 Loss G: 0.0014877207577228546\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 12016\n",
      "W with GP: -0.1755479872226715 Loss G: 0.060251861810684204\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 12512\n",
      "W with GP: -0.32663989067077637 Loss G: 0.3064720928668976\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 13008\n",
      "W with GP: 0.18494325876235962 Loss G: -0.41636744141578674\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 13504\n",
      "W with GP: 0.24452060461044312 Loss G: 0.06131464242935181\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 14000\n",
      "W with GP: -1.9476144313812256 Loss G: 2.452653646469116\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 14016\n",
      "W with GP: 1.884299397468567 Loss G: -1.0235823392868042\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 14512\n",
      "W with GP: 0.0961228758096695 Loss G: -0.11853916943073273\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 15008\n",
      "W with GP: -0.6992077827453613 Loss G: 0.3607730269432068\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 15504\n",
      "W with GP: -0.12256929278373718 Loss G: 0.17564597725868225\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 16000\n",
      "W with GP: 0.06397892534732819 Loss G: 0.2717421054840088\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 16016\n",
      "W with GP: 1.0383528470993042 Loss G: -0.23448024690151215\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 16512\n",
      "W with GP: -0.32129204273223877 Loss G: 0.42000553011894226\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 17008\n",
      "W with GP: -1.716080904006958 Loss G: 2.2957282066345215\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 17504\n",
      "W with GP: 0.10724706947803497 Loss G: -0.0022059977054595947\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 18000\n",
      "W with GP: -0.013236768543720245 Loss G: -0.15388688445091248\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 18016\n",
      "W with GP: -0.1266484260559082 Loss G: -0.041464682668447495\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 18512\n",
      "W with GP: 0.13316982984542847 Loss G: -0.10352825373411179\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 19008\n",
      "W with GP: 1.0287052392959595 Loss G: -1.1385389566421509\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 19504\n",
      "W with GP: 0.3277338743209839 Loss G: -0.21390995383262634\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 20000\n",
      "W with GP: 0.12837208807468414 Loss G: -0.06136815994977951\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 20016\n",
      "W with GP: -0.03379073739051819 Loss G: 0.10557427257299423\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 20512\n",
      "W with GP: 0.06451065838336945 Loss G: 0.13052666187286377\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 21008\n",
      "W with GP: 0.33027175068855286 Loss G: -0.3541210889816284\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 21504\n",
      "W with GP: -0.13905684649944305 Loss G: -0.16634473204612732\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 22000\n",
      "W with GP: -0.2171863615512848 Loss G: 0.17144735157489777\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 22016\n",
      "W with GP: 1.1905503273010254 Loss G: -0.8279210925102234\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 22512\n",
      "W with GP: -0.3737885355949402 Loss G: 1.2388807535171509\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 23008\n",
      "W with GP: 0.18287764489650726 Loss G: 0.6055475473403931\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 23504\n",
      "W with GP: -0.4332045912742615 Loss G: 0.8361692428588867\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 24000\n",
      "W with GP: -0.5905326008796692 Loss G: 1.013127326965332\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 24016\n",
      "W with GP: -0.6005248427391052 Loss G: 0.5588523149490356\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 24512\n",
      "W with GP: -0.03353827819228172 Loss G: 0.7106953859329224\n",
      "\n",
      "Res: 4 Fade_in: False Iter: 25008\n",
      "W with GP: 0.6573053002357483 Loss G: -0.2979733943939209\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def training():\n",
    "\n",
    "\n",
    "    # Setup Adam optimizers for both G and D\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "    optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "    \n",
    "    \n",
    "    training = True\n",
    "    current_data = 0\n",
    "    res = 4\n",
    "    fade_in = False\n",
    "    \n",
    "    while(training):\n",
    "        steps = 800000\n",
    "        loader = iter(data_loaders[current_data])\n",
    "        count = 0\n",
    "        \n",
    "    \n",
    "        while count < steps:\n",
    "            try:\n",
    "                img = loader.next()\n",
    "            except StopIteration:\n",
    "                loader = iter(data_loaders[current_data])\n",
    "                img = loader.next()\n",
    "            if fade_in:\n",
    "                alpha = -1\n",
    "               \n",
    "            else:\n",
    "                alpha = count/steps\n",
    "                \n",
    "            mini_batch = len(img[0])\n",
    "    \n",
    "            real_imgs = img[0].to(device)\n",
    "        \n",
    "            netD.zero_grad()\n",
    "            \n",
    "#             Discriminator loss on real images\n",
    "\n",
    "            output_real = netD(real_imgs, alpha=alpha, res=res).squeeze()\n",
    "\n",
    "            \n",
    "#             Discriminator loss on fake images\n",
    "            \n",
    "            noise = torch.randn(mini_batch, nz, 1, 1, device=device)\n",
    "            fake_imgs = netG(noise, res=res, alpha=alpha)\n",
    "            output_fake = netD(fake_imgs.detach(), alpha=alpha, res=res).squeeze()\n",
    "            \n",
    "\n",
    "#             Gradient Penalty\n",
    "            \n",
    "            gp_alpha = torch.randn(mini_batch, 1, 1, 1, device = device)\n",
    "            interp = gp_alpha * real_imgs + ((1-gp_alpha) * fake_imgs.detach())\n",
    "            interp.requires_grad = True\n",
    "            model_interp = netD(interp, alpha = alpha, res = res)\n",
    "            grads = torch.autograd.grad(outputs=model_interp, inputs=interp,\n",
    "                          grad_outputs=torch.ones(model_interp.size()).to(device),\n",
    "                          create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "            grads = torch.square(grads)\n",
    "            grads = torch.sum(grads, dim = [1,2,3])\n",
    "            grads = torch.sqrt(grads)\n",
    "            grads = grads - 1\n",
    "            grads = torch.square(grads)\n",
    "            grad_pen = grads * lambda_gp\n",
    "            \n",
    "#             Extra small penalty\n",
    "\n",
    "            penalty = torch.square(output_real)\n",
    "            penalty = penalty * .001\n",
    "        \n",
    "#             Calculating entire loss and taking step\n",
    "            \n",
    "            loss_D = torch.mean(output_fake - output_real + grad_pen + penalty)\n",
    "            \n",
    "            \n",
    "            loss_D.backward()\n",
    "            \n",
    "            optimizerD.step()\n",
    "#             loss_D = loss_fake + loss_real + grad_pen\n",
    "            \n",
    "                \n",
    "                \n",
    "            netG.zero_grad()\n",
    "            \n",
    "#             Generator loss on created batch\n",
    "\n",
    "            output = netD(fake_imgs, alpha=alpha, res=res).squeeze()\n",
    "            loss_G = -torch.mean(output)\n",
    "            loss_G.backward()    \n",
    "\n",
    "            optimizerG.step()\n",
    "\n",
    "#             Training Stats\n",
    "                \n",
    "            count += mini_batch\n",
    "            if count %500 <= mini_batch:\n",
    "                print(\"Res:\", res, \"Fade_in:\", fade_in, \"Iter:\",count)\n",
    "                print(\"W with GP:\", loss_D.item(),  \"Loss G:\", loss_G.item())\n",
    "#                 print(\"Loss real:\", loss_real.item(), \"Loss Fake:\", loss_fake.item())\n",
    "                print()\n",
    "        \n",
    "        \n",
    "        if fade_in == False:\n",
    "            fade_in = True\n",
    "            current_data += 1\n",
    "            res = res * 2\n",
    "        else:\n",
    "            fade_in = False\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "training()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 4, 4]) tensor(0.) tensor(1.)\n",
      "torch.Size([64, 4, 4, 3]) tensor(0.4584) tensor(0.7641)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# OldRange = (OldMax - OldMin)  \n",
    "# NewRange = (NewMax - NewMin)  \n",
    "# NewValue = (((OldValue - OldMin) * NewRange) / OldRange) + NewMin\n",
    "\n",
    "\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "with torch.no_grad():\n",
    "    guess = netG(fixed_noise, res = 4, alpha=-1)\n",
    "    guess = guess.cpu()\n",
    "    old_min = torch.min(guess)\n",
    "    old_max = torch.max(guess)\n",
    "    old_range = old_max - old_min\n",
    "    new_range = 1 - 0\n",
    "    guess = (((guess - old_min)*new_range)/ old_range) + 0\n",
    "#     guess = guess.float()\n",
    "    print(guess.shape, torch.min(guess), torch.max(guess))\n",
    "#     guess = ((guess *.5 ) + .5)\n",
    "    guess = guess.permute(0,2,3,1)\n",
    "    print(guess.shape, torch.min(guess[0]), torch.max(guess[0]))\n",
    "\n",
    "\n",
    "    fig = plt.figure(figsize=(4,4))\n",
    "    for i in range(16):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        plt.imshow(guess[i, :, :])\n",
    "        plt.axis('off')\n",
    "#     plt.savefig('end_train.png', dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First get it working without mixed precision  \n",
    "\n",
    "\n",
    "\n",
    "# #             Discriminator loss on real images\n",
    "#             with torch.cuda.amp.autocast(): \n",
    "#                 output_real = netD(real_imgs, alpha=alpha, res=res).squeeze()\n",
    "#                 loss_real = -torch.mean(output_real)\n",
    "# #                 print(loss_real)\n",
    "# #             print(loss_real) \n",
    "#             scalerD.scale(loss_real).backward()\n",
    "# #             print(loss_real)\n",
    "\n",
    "            \n",
    "#             noise = torch.randn(mini_batch, nz, 1, 1, device=device)\n",
    "            \n",
    "# #             Discriminator loss on fake images\n",
    "#             with torch.cuda.amp.autocast():\n",
    "            \n",
    "#                 fake_imgs = netG(noise, res=res, alpha=alpha)\n",
    "#                 output_fake = netD(fake_imgs.detach(), alpha=alpha, res=res).squeeze()\n",
    "                \n",
    "#                 loss_fake = torch.mean(output_fake)\n",
    "            \n",
    "#             scalerD.scale(loss_fake).backward()\n",
    "            \n",
    "# #             Gradient Penalty\n",
    "#             with torch.cuda.amp.autocast():\n",
    "#                 gp_alpha = torch.randn(mini_batch, 1, 1, 1, device = device)\n",
    "#                 gp_alpha = gp_alpha.expand(real_imgs.size(0), real_imgs.size(1), real_imgs.size(2), real_imgs.size(3))\n",
    "#                 interp = gp_alpha * real_imgs + ((1-gp_alpha) * fake_imgs.detach())\n",
    "#                 interp.requires_grad = True\n",
    "#                 model_interp = netD(interp, alpha = alpha, res = res)\n",
    "            \n",
    "#             grads = torch.autograd.grad(outputs=model_interp, inputs=interp,\n",
    "#                               grad_outputs=torch.ones(model_interp.size()).to(device),\n",
    "#                               create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "            \n",
    "# #             Do I need to unscale the gradients for calculating gp for wasserstein?\n",
    "# #             print(scaled_grad_params.shape)\n",
    "# #             inv_scale = 1./scalerD.get_scale()\n",
    "# #             grads = [p * inv_scale for p in scaled_grad_params]\n",
    "# #             print(grads)\n",
    "            \n",
    "#             with torch.cuda.amp.autocast():\n",
    "#                 grads = ((grads.norm(2, dim=1) - 1) ** 2).mean()\n",
    "#                 grads += 1e-16\n",
    "#                 grad_pen = grads * lambda_gp\n",
    "# #                 print(grad_pen.item())\n",
    "            \n",
    "#             scalerD.scale(grad_pen).backward()\n",
    "# #             print(grad_pen.item())\n",
    "# #             return\n",
    "#             scalerD.step(optimizerD)\n",
    "#             scalerD.update()\n",
    "#             loss_D = loss_fake + loss_real + grad_pen\n",
    "            \n",
    "                \n",
    "                \n",
    "#             netG.zero_grad()\n",
    "            \n",
    "\n",
    "#             with torch.cuda.amp.autocast():\n",
    "#                 output = netD(fake_imgs, alpha=alpha, res=res).squeeze()\n",
    "#                 loss_G = -torch.mean(output)\n",
    "#             scalerG.scale(loss_G).backward()    \n",
    "\n",
    "#             scalerG.step(optimizerG)\n",
    "#             scalerG.update()\n",
    "                \n",
    "            \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
