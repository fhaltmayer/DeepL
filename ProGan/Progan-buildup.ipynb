{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  999\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "#%matplotlib inline\n",
    "import argparse\n",
    "import gc\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "manualSeed = 999\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "\n",
    "# Root directory for dataset\n",
    "dataroot = \"/media/fico/Data/Celeba/CelebAMask-HQ\"\n",
    "\n",
    "# Number of workers for dataloader\n",
    "workers = 2\n",
    "\n",
    "# Batch size during training\n",
    "\n",
    "\n",
    "\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "nz = 512\n",
    "\n",
    "# Size of feature maps in generator\n",
    "ngf = 4\n",
    "\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr = 0.0002\n",
    "\n",
    "# Beta1 hyperparam for Adam optimizers\n",
    "beta1 = 0\n",
    "\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = 1\n",
    "\n",
    "lambda_gp = 10\n",
    "\n",
    "d_ratio = 1\n",
    "\n",
    "img_batch_size = [(4,16),(8,16),(16,16),(32,16),(64,16),(128,16)]\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.8980) tensor(0.8510)\n"
     ]
    }
   ],
   "source": [
    "data_loaders = []\n",
    "for img_size, batch_size in img_batch_size:\n",
    "    dataset = dset.ImageFolder(root=dataroot,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(img_size),\n",
    "                               transforms.CenterCrop(img_size),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "\n",
    "    dataload = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                             shuffle=True, num_workers=workers, drop_last=True)\n",
    "    data_loaders.append(dataload)\n",
    "\n",
    "x = next(iter(data_loaders[0]))\n",
    "print(torch.min(x[0]), torch.max(x[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator Code\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "#         self.added = nn.ModuleList([])\n",
    "        self.up_samp = nn.Upsample(scale_factor = 2)\n",
    "#         4\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Upsample(scale_factor = 4),\n",
    "            nn.Conv2d(nz, 512, 3, 1, 1),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.LeakyReLU(True),)\n",
    "        self.end = nn.Conv2d(512, 3, 1, 1, 0)\n",
    "        \n",
    "#         8\n",
    "        self.block1 = nn.Sequential(\n",
    "            self.up_samp,\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.LeakyReLU(True),)\n",
    "        self.end1 = nn.Conv2d(512, 3, 1, 1, 0)\n",
    "        \n",
    "#         16\n",
    "        self.block2 = nn.Sequential(\n",
    "            self.up_samp,\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.LeakyReLU(True),)\n",
    "        self.end2 = nn.Conv2d(512, 3, 1, 1, 0)\n",
    "        \n",
    "#         32\n",
    "        self.block3 = nn.Sequential(\n",
    "            self.up_samp,\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.LeakyReLU(True),)\n",
    "        self.end3 = nn.Conv2d(512, 3, 1, 1, 0)\n",
    "        \n",
    "#         64\n",
    "        self.block4 = nn.Sequential(\n",
    "            self.up_samp,\n",
    "            nn.Conv2d(512, 256, 3, 1, 1),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Conv2d(256, 256, 3, 1, 1),\n",
    "            nn.LeakyReLU(True),)\n",
    "        self.end4 = nn.Conv2d(256, 3, 1, 1, 0)\n",
    "        \n",
    "#          128\n",
    "        self.block5 = nn.Sequential(\n",
    "            self.up_samp,\n",
    "            nn.Conv2d(256, 128, 3, 1, 1),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Conv2d(128, 128, 3, 1, 1),\n",
    "            nn.LeakyReLU(True),)\n",
    "        self.end5 = nn.Conv2d(128, 3, 1, 1, 0)\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, input, res, alpha):\n",
    "        \n",
    "        \n",
    "        \n",
    "        if res == 4:\n",
    "            output = self.block(input)\n",
    "            output = self.end(output)\n",
    "        elif res == 8:\n",
    "            output = self.block(input)\n",
    "            output_old = self.up_samp(output)\n",
    "            output_old = self.end(output_old)\n",
    "            output = self.block1(output)\n",
    "            output = self.end1(output)\n",
    "            output = alpha*output + (1-alpha)*output_old\n",
    "        elif res == 16:\n",
    "            output = self.block(input)\n",
    "            output = self.block1(output)\n",
    "            output_old = self.up_samp(output)\n",
    "            output_old = self.end1(output_old)\n",
    "            output = self.block2(output)\n",
    "            output = self.end2(output)\n",
    "            output = alpha*output + (1-alpha)*output_old\n",
    "        elif res == 32:\n",
    "            output = self.block(input)\n",
    "            output = self.block1(output)\n",
    "            output = self.block2(output)\n",
    "            output_old = self.up_samp(output)\n",
    "            output_old = self.end2(output_old)\n",
    "            output = self.block3(output)\n",
    "            output = self.end3(output)\n",
    "            output = alpha*output + (1-alpha)*output_old\n",
    "        elif res == 64:\n",
    "            output = self.block(input)\n",
    "            output = self.block1(output)\n",
    "            output = self.block2(output)\n",
    "            output = self.block3(output)\n",
    "            output_old = self.up_samp(output)\n",
    "            output_old = self.end3(output_old)\n",
    "            output = self.block4(output)\n",
    "            output = self.end4(output)\n",
    "            output = alpha*output + (1-alpha)*output_old\n",
    "        elif res == 128:\n",
    "            output = self.block(input)\n",
    "            output = self.block1(output)\n",
    "            output = self.block2(output)\n",
    "            output = self.block3(output)\n",
    "            output = self.block4(output)\n",
    "            output_old = self.up_samp(output)\n",
    "            output_old = self.end4(output_old)\n",
    "            output = self.block5(output)\n",
    "            output = self.end5(output)\n",
    "            output = alpha*output + (1-alpha)*output_old\n",
    "            \n",
    "#         print(output.shape) \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "#         self.added = nn.ModuleList([])\n",
    "        self.down_samp = nn.AvgPool2d(2)\n",
    "    \n",
    "#         4\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Conv2d(512, 512, 4, 1, 0),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, 1))\n",
    "        self.start = nn.Conv2d(3, 512, 1, 1, 0)\n",
    "        \n",
    "#         8\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.LeakyReLU(True),\n",
    "            self.down_samp,)\n",
    "        self.start1 = nn.Conv2d(3, 512, 1, 1, 0)\n",
    "\n",
    "#         16\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.LeakyReLU(True),\n",
    "            self.down_samp,)\n",
    "        self.start2 = nn.Conv2d(3, 512, 1, 1, 0)\n",
    "        \n",
    "#         32\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.LeakyReLU(True),\n",
    "            self.down_samp,)\n",
    "        self.start3 = nn.Conv2d(3, 512, 1, 1, 0)\n",
    "        \n",
    "#         64\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, 3, 1, 1),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Conv2d(256, 512, 3, 1, 1),\n",
    "            nn.LeakyReLU(True),\n",
    "            self.down_samp,)\n",
    "        self.start4 = nn.Conv2d(3, 256, 1, 1, 0)\n",
    "        \n",
    "#         128\n",
    "        self.block5= nn.Sequential(\n",
    "            nn.Conv2d(128, 128, 3, 1, 1),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Conv2d(128, 256, 3, 1, 1),\n",
    "            nn.LeakyReLU(True),\n",
    "            self.down_samp,)\n",
    "        self.start5 = nn.Conv2d(3, 128, 1, 1, 0)\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, input, res, alpha):   \n",
    "        \n",
    "        if res == 4:\n",
    "            output = self.start(input)\n",
    "            output = self.block(output)\n",
    "            \n",
    "        elif res == 8:\n",
    "            output = self.start1(input)\n",
    "            output_old = self.down_samp(input)\n",
    "            output_old = self.start(output_old)\n",
    "            output = self.block1(output)\n",
    "            output = alpha*output + (1-alpha)*output_old\n",
    "            output = self.block(output)\n",
    "        elif res == 16:\n",
    "            output = self.start2(input)\n",
    "            output_old = self.down_samp(input)\n",
    "            output_old = self.start1(output_old)\n",
    "            output = self.block2(output)\n",
    "            output = alpha*output + (1-alpha)*output_old\n",
    "            output = self.block1(output)\n",
    "            output = self.block(output)\n",
    "        elif res == 32:\n",
    "            output = self.start3(input)\n",
    "            output_old = self.down_samp(input)\n",
    "            output_old = self.start2(output_old)\n",
    "            output = self.block3(output)\n",
    "            output = alpha*output + (1-alpha)*output_old\n",
    "            output = self.block2(output)\n",
    "            output = self.block1(output)\n",
    "            output = self.block(output)\n",
    "        elif res == 64:\n",
    "            output = self.start4(input)\n",
    "            output_old = self.down_samp(input)\n",
    "            output_old = self.start3(output_old)\n",
    "            output = self.block4(output)\n",
    "            output = alpha*output + (1-alpha)*output_old\n",
    "            output = self.block3(output)\n",
    "            output = self.block2(output)\n",
    "            output = self.block1(output)\n",
    "            output = self.block(output)\n",
    "        elif res == 128:\n",
    "            output = self.start5(input)\n",
    "            output_old = self.down_samp(input)\n",
    "            output_old = self.start4(output_old)\n",
    "            output = self.block5(output)\n",
    "            output = alpha*output + (1-alpha)*output_old\n",
    "            output = self.block4(output)\n",
    "            output = self.block3(output)\n",
    "            output = self.block2(output)\n",
    "            output = self.block1(output)\n",
    "            output = self.block(output)\n",
    "            \n",
    "#         print(output.shape) \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "netG = Generator(ngpu).to(device)\n",
    "netD = Discriminator(ngpu).to(device)\n",
    "# with torch.cuda.amp.autocast(): \n",
    "#     test = torch.randn(16, nz, 1, 1, device=device)\n",
    "#     test1 = torch.randn(16, 3, 128, 128, device=device)\n",
    "    \n",
    "#     temp = netG(test, res = 8, alpha = 1)\n",
    "#     print(temp.shape)\n",
    "#     netD(temp, res = 8, alpha = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Finish W_loss\n",
    "def w_loss(discriminator, real_imgs, fake_imgs, alpha, res):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 16 W with GP: -6.549338340759277 Loss real: 0.72705078125 Loss Fake: -7.9453125 Loss G: 7.9453125\n",
      "Iter: 512 W with GP: -13.32442569732666 Loss real: 3.21484375 Loss Fake: -17.0 Loss G: 24.6875\n",
      "Iter: 1008 W with GP: 1.488718867301941 Loss real: -0.6328125 Loss Fake: 0.9345703125 Loss G: 2.607421875\n",
      "Iter: 1504 W with GP: -8.001967430114746 Loss real: -1.6396484375 Loss Fake: -6.63671875 Loss G: 10.4140625\n",
      "Iter: 2000 W with GP: 14.932357788085938 Loss real: 0.10888671875 Loss Fake: 14.21875 Loss G: 8.5\n",
      "Iter: 16 W with GP: nan Loss real: -0.44140625 Loss Fake: nan Loss G: nan\n",
      "Iter: 512 W with GP: nan Loss real: -1.46484375 Loss Fake: nan Loss G: nan\n",
      "Iter: 1008 W with GP: nan Loss real: -1.7158203125 Loss Fake: nan Loss G: nan\n",
      "Iter: 1504 W with GP: nan Loss real: -2.58203125 Loss Fake: nan Loss G: nan\n",
      "Iter: 2000 W with GP: nan Loss real: -5.5625 Loss Fake: nan Loss G: nan\n",
      "Iter: 16 W with GP: nan Loss real: -2.787109375 Loss Fake: nan Loss G: nan\n",
      "Iter: 512 W with GP: nan Loss real: 4.58203125 Loss Fake: nan Loss G: nan\n",
      "Iter: 1008 W with GP: nan Loss real: 10.0625 Loss Fake: nan Loss G: nan\n",
      "Iter: 1504 W with GP: nan Loss real: 13.3515625 Loss Fake: nan Loss G: nan\n",
      "Iter: 2000 W with GP: nan Loss real: 6.69140625 Loss Fake: nan Loss G: nan\n",
      "Iter: 16 W with GP: nan Loss real: 33.46875 Loss Fake: nan Loss G: nan\n",
      "Iter: 512 W with GP: nan Loss real: 11.1328125 Loss Fake: nan Loss G: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-893dc9e8d087>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-67-893dc9e8d087>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             grads = torch.autograd.grad(outputs=model_interp, inputs=interp,\n\u001b[0;32m---> 80\u001b[0;31m                               \u001b[0mgrad_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_interp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                               create_graph=True, retain_graph=True, only_inputs=True)[0]\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def training():\n",
    "#     Initialize BCELoss function\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "    # Create batch of latent vectors that we will use to visualize\n",
    "    #  the progression of the generator\n",
    "    fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "\n",
    "    # Establish convention for real and fake labels during training\n",
    "    real_label = 1.\n",
    "    fake_label = 0\n",
    "    \n",
    "    label = torch.tensor(1)\n",
    "\n",
    "    # Setup Adam optimizers for both G and D\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "    optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "    scalerD = torch.cuda.amp.GradScaler()\n",
    "    scalerG = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    \n",
    "    training = True\n",
    "    current = 0\n",
    "    res = 4\n",
    "#     mini_batch = 16\n",
    "    \n",
    "    while(training):\n",
    "        steps = 2000\n",
    "        loader = iter(data_loaders[current])\n",
    "        count = 0\n",
    "        \n",
    "        fade = False\n",
    "    \n",
    "        while count < steps:\n",
    "            try:\n",
    "                img = loader.next()\n",
    "            except StopIteration:\n",
    "                loader = iter(data_loaders[current])\n",
    "                img = loader.next()\n",
    "            \n",
    "            alpha = count/steps\n",
    "            mini_batch = len(img[0])\n",
    "            \n",
    "            real_imgs = img[0].to(device)\n",
    "            \n",
    "            netD.zero_grad()\n",
    "\n",
    "            \n",
    "# First get it working without mixed precision  \n",
    "\n",
    "\n",
    "\n",
    "# #             Discriminator loss on real images\n",
    "#             with torch.cuda.amp.autocast(): \n",
    "#                 output_real = netD(real_imgs, alpha=alpha, res=res).squeeze()\n",
    "#                 loss_real = -torch.mean(output_real)\n",
    "# #                 print(loss_real)\n",
    "# #             print(loss_real) \n",
    "#             scalerD.scale(loss_real).backward()\n",
    "# #             print(loss_real)\n",
    "\n",
    "            \n",
    "#             noise = torch.randn(mini_batch, nz, 1, 1, device=device)\n",
    "            \n",
    "# #             Discriminator loss on fake images\n",
    "#             with torch.cuda.amp.autocast():\n",
    "            \n",
    "#                 fake_imgs = netG(noise, res=res, alpha=alpha)\n",
    "#                 output_fake = netD(fake_imgs.detach(), alpha=alpha, res=res).squeeze()\n",
    "                \n",
    "#                 loss_fake = torch.mean(output_fake)\n",
    "            \n",
    "#             scalerD.scale(loss_fake).backward()\n",
    "            \n",
    "# #             Gradient Penalty\n",
    "#             with torch.cuda.amp.autocast():\n",
    "#                 gp_alpha = torch.randn(mini_batch, 1, 1, 1, device = device)\n",
    "#                 gp_alpha = gp_alpha.expand(real_imgs.size(0), real_imgs.size(1), real_imgs.size(2), real_imgs.size(3))\n",
    "#                 interp = gp_alpha * real_imgs + ((1-gp_alpha) * fake_imgs.detach())\n",
    "#                 interp.requires_grad = True\n",
    "#                 model_interp = netD(interp, alpha = alpha, res = res)\n",
    "            \n",
    "#             grads = torch.autograd.grad(outputs=model_interp, inputs=interp,\n",
    "#                               grad_outputs=torch.ones(model_interp.size()).to(device),\n",
    "#                               create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "            \n",
    "# #             Do I need to unscale the gradients for calculating gp for wasserstein?\n",
    "# #             print(scaled_grad_params.shape)\n",
    "# #             inv_scale = 1./scalerD.get_scale()\n",
    "# #             grads = [p * inv_scale for p in scaled_grad_params]\n",
    "# #             print(grads)\n",
    "            \n",
    "#             with torch.cuda.amp.autocast():\n",
    "#                 grads = ((grads.norm(2, dim=1) - 1) ** 2).mean()\n",
    "#                 grads += 1e-16\n",
    "#                 grad_pen = grads * lambda_gp\n",
    "# #                 print(grad_pen.item())\n",
    "            \n",
    "#             scalerD.scale(grad_pen).backward()\n",
    "# #             print(grad_pen.item())\n",
    "# #             return\n",
    "#             scalerD.step(optimizerD)\n",
    "#             scalerD.update()\n",
    "#             loss_D = loss_fake + loss_real + grad_pen\n",
    "            \n",
    "                \n",
    "                \n",
    "#             netG.zero_grad()\n",
    "            \n",
    "\n",
    "#             with torch.cuda.amp.autocast():\n",
    "#                 output = netD(fake_imgs, alpha=alpha, res=res).squeeze()\n",
    "#                 loss_G = -torch.mean(output)\n",
    "#             scalerG.scale(loss_G).backward()    \n",
    "\n",
    "#             scalerG.step(optimizerG)\n",
    "#             scalerG.update()\n",
    "                \n",
    "            \n",
    "    \n",
    "                \n",
    "            count += mini_batch\n",
    "            if count %500 <= mini_batch:\n",
    "                print(\"Iter:\",count,\"W with GP:\", loss_D.item(), \"Loss real:\", loss_real.item(), \"Loss Fake:\", loss_fake.item(), \"Loss G:\", loss_G.item())\n",
    "        current += 1\n",
    "        res = res * 2\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "training()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
