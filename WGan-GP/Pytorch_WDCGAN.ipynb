{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f5f00ab7d80>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "#%matplotlib inline\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "manualSeed = 999\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataroot = \"/media/fico/Data/Celeba/CelebAMask-HQ\"\n",
    "\n",
    "workers = 4\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "image_size = 64\n",
    "\n",
    "nz = 100\n",
    "\n",
    "nf = 64\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "lr = 0.0001\n",
    "\n",
    "betas = (0, .9)\n",
    "\n",
    "lambda_gp = 10\n",
    "\n",
    "d_ratio = 5\n",
    "\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235\n"
     ]
    }
   ],
   "source": [
    "# We can use an image folder dataset the way we have it setup.\n",
    "# Create the dataset\n",
    "dataset = dset.ImageFolder(root=dataroot,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(image_size),\n",
    "                               transforms.CenterCrop(image_size),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "# Create the dataloader\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=workers)\n",
    "\n",
    "print(len(dataloader))\n",
    "# Decide which device we want to run on\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator Code\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d( nz, nf * 16, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(nf * 16),\n",
    "            nn.ReLU(True),)\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(nf * 16, nf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(nf * 8),\n",
    "            nn.ReLU(True),)\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d( nf * 8, nf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(nf * 4),\n",
    "            nn.ReLU(True),)\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d( nf * 4, nf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(nf*2),\n",
    "            nn.ReLU(True),)\n",
    "        self.block5 = nn.Sequential(\n",
    "            nn.ConvTranspose2d( nf *2, 3, 4, 2, 1, bias=False),\n",
    "            nn.Tanh())\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.block1(input)\n",
    "        output = self.block2(output)\n",
    "        output = self.block3(output)\n",
    "        output = self.block4(output)\n",
    "        output = self.block5(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(nc, nf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(nf, nf * 2, 4, 2, 1, bias=False),\n",
    "            nn.InstanceNorm2d(nf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(nf * 2, nf * 4, 4, 2, 1, bias=False),\n",
    "            nn.InstanceNorm2d(nf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(nf * 4, nf * 8, 4, 2, 1, bias=False),\n",
    "            nn.InstanceNorm2d(nf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(nf * 8, 1, 4, 1, 0, bias=False),\n",
    "        )\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (main): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (9): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netD = Discriminator().to(device)\n",
    "netG = Generator().to(device)\n",
    "netG.apply(weights_init)\n",
    "netD.apply(weights_init)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "\n",
    "epo = 0\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=betas)\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./Saved_Models/WGAN_CheckPoint_Epoch:_\" + str(19)\n",
    "check = torch.load(path)\n",
    "netD.load_state_dict(check['netD_state_dict'])\n",
    "netG.load_state_dict(check['netG_state_dict'])\n",
    "optimizerD.load_state_dict(check['optimizerD_state_dict'])\n",
    "optimizerG.load_state_dict(check['optimizerG_state_dict'])\n",
    "epo = check['epoch']\n",
    "epo += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Start\n",
      "\n",
      "Epoch: 20 Iteration: 0\n",
      "WGAN GP Loss: -47.113319396972656 G Loss: 33.14257049560547\n",
      "\n",
      "Epoch: 20 Iteration: 1280\n",
      "WGAN GP Loss: -46.35020446777344 G Loss: 31.81186294555664\n",
      "\n",
      "Epoch: 20 Iteration: 2560\n",
      "WGAN GP Loss: -46.919700622558594 G Loss: 34.638763427734375\n",
      "\n",
      "Epoch: 20 Iteration: 3840\n",
      "WGAN GP Loss: -45.11269760131836 G Loss: 31.40787696838379\n",
      "\n",
      "Epoch: 20 Iteration: 5120\n",
      "WGAN GP Loss: -47.773681640625 G Loss: 32.78889846801758\n",
      "\n",
      "Epoch: 20 Iteration: 6400\n",
      "WGAN GP Loss: -52.816436767578125 G Loss: 34.75325012207031\n",
      "\n",
      "Epoch: 20 Iteration: 7680\n",
      "WGAN GP Loss: -50.3585090637207 G Loss: 34.274566650390625\n",
      "\n",
      "Epoch: 20 Iteration: 8960\n",
      "WGAN GP Loss: -49.76044464111328 G Loss: 34.696250915527344\n",
      "\n",
      "Epoch: 20 Iteration: 10240\n",
      "WGAN GP Loss: -50.89025115966797 G Loss: 33.42718505859375\n",
      "\n",
      "Epoch: 20 Iteration: 11520\n",
      "WGAN GP Loss: -45.4417724609375 G Loss: 33.327205657958984\n",
      "\n",
      "Epoch: 20 Iteration: 12800\n",
      "WGAN GP Loss: -49.05784606933594 G Loss: 31.94373321533203\n",
      "\n",
      "Epoch: 20 Iteration: 14080\n",
      "WGAN GP Loss: -50.928993225097656 G Loss: 35.617008209228516\n",
      "\n",
      "Epoch: 20 Iteration: 15360\n",
      "WGAN GP Loss: -47.56052017211914 G Loss: 33.62405776977539\n",
      "\n",
      "Epoch: 20 Iteration: 16640\n",
      "WGAN GP Loss: -38.90269470214844 G Loss: 29.859769821166992\n",
      "\n",
      "Epoch: 20 Iteration: 17920\n",
      "WGAN GP Loss: -46.259300231933594 G Loss: 36.05828857421875\n",
      "\n",
      "Epoch: 20 Iteration: 19200\n",
      "WGAN GP Loss: -55.318912506103516 G Loss: 32.84075927734375\n",
      "\n",
      "Epoch: 20 Iteration: 20480\n",
      "WGAN GP Loss: -45.92866134643555 G Loss: 30.13420867919922\n",
      "\n",
      "Epoch: 20 Iteration: 21760\n",
      "WGAN GP Loss: -46.988136291503906 G Loss: 32.56611633300781\n",
      "\n",
      "Epoch: 20 Iteration: 23040\n",
      "WGAN GP Loss: -48.301368713378906 G Loss: 33.06415939331055\n",
      "\n",
      "Epoch: 20 Iteration: 24320\n",
      "WGAN GP Loss: -50.50784683227539 G Loss: 33.70033264160156\n",
      "\n",
      "Epoch: 20 Iteration: 25600\n",
      "WGAN GP Loss: -46.66956329345703 G Loss: 31.910812377929688\n",
      "\n",
      "Epoch: 20 Iteration: 26880\n",
      "WGAN GP Loss: -43.168155670166016 G Loss: 32.165077209472656\n",
      "\n",
      "Epoch: 20 Iteration: 28160\n",
      "WGAN GP Loss: -43.834716796875 G Loss: 30.94652557373047\n",
      "\n",
      "Epoch: 20 Iteration: 29440\n",
      "WGAN GP Loss: -46.7131233215332 G Loss: 33.21160125732422\n",
      "\n",
      "Epoch: 21 Iteration: 0\n",
      "WGAN GP Loss: -44.33108901977539 G Loss: 32.14311218261719\n",
      "\n",
      "Epoch: 21 Iteration: 1280\n",
      "WGAN GP Loss: -47.983768463134766 G Loss: 34.341644287109375\n",
      "\n",
      "Epoch: 21 Iteration: 2560\n",
      "WGAN GP Loss: -50.340118408203125 G Loss: 32.97987365722656\n",
      "\n",
      "Epoch: 21 Iteration: 3840\n",
      "WGAN GP Loss: -46.98982620239258 G Loss: 32.738746643066406\n",
      "\n",
      "Epoch: 21 Iteration: 5120\n",
      "WGAN GP Loss: -50.76051330566406 G Loss: 33.3490104675293\n",
      "\n",
      "Epoch: 21 Iteration: 6400\n",
      "WGAN GP Loss: -50.59968185424805 G Loss: 33.26698303222656\n",
      "\n",
      "Epoch: 21 Iteration: 7680\n",
      "WGAN GP Loss: -46.55486297607422 G Loss: 30.313400268554688\n",
      "\n",
      "Epoch: 21 Iteration: 8960\n",
      "WGAN GP Loss: -49.94266128540039 G Loss: 33.25617980957031\n",
      "\n",
      "Epoch: 21 Iteration: 10240\n",
      "WGAN GP Loss: -48.620269775390625 G Loss: 32.53313446044922\n",
      "\n",
      "Epoch: 21 Iteration: 11520\n",
      "WGAN GP Loss: -52.885868072509766 G Loss: 33.88829040527344\n",
      "\n",
      "Epoch: 21 Iteration: 12800\n",
      "WGAN GP Loss: -46.323726654052734 G Loss: 32.696556091308594\n",
      "\n",
      "Epoch: 21 Iteration: 14080\n",
      "WGAN GP Loss: -43.12813186645508 G Loss: 31.196611404418945\n",
      "\n",
      "Epoch: 21 Iteration: 15360\n",
      "WGAN GP Loss: -46.989681243896484 G Loss: 31.090627670288086\n",
      "\n",
      "Epoch: 21 Iteration: 16640\n",
      "WGAN GP Loss: -45.76076126098633 G Loss: 30.737045288085938\n",
      "\n",
      "Epoch: 21 Iteration: 17920\n",
      "WGAN GP Loss: -45.11279296875 G Loss: 32.40891647338867\n",
      "\n",
      "Epoch: 21 Iteration: 19200\n",
      "WGAN GP Loss: -48.987857818603516 G Loss: 33.00141143798828\n",
      "\n",
      "Epoch: 21 Iteration: 20480\n",
      "WGAN GP Loss: -46.73345947265625 G Loss: 29.79375457763672\n",
      "\n",
      "Epoch: 21 Iteration: 21760\n",
      "WGAN GP Loss: -43.59014129638672 G Loss: 28.65449333190918\n",
      "\n",
      "Epoch: 21 Iteration: 23040\n",
      "WGAN GP Loss: -47.782196044921875 G Loss: 30.803321838378906\n",
      "\n",
      "Epoch: 21 Iteration: 24320\n",
      "WGAN GP Loss: -47.78009796142578 G Loss: 33.85577392578125\n",
      "\n",
      "Epoch: 21 Iteration: 25600\n",
      "WGAN GP Loss: -52.280696868896484 G Loss: 35.43534851074219\n",
      "\n",
      "Epoch: 21 Iteration: 26880\n",
      "WGAN GP Loss: -49.21764373779297 G Loss: 32.80875778198242\n",
      "\n",
      "Epoch: 21 Iteration: 28160\n",
      "WGAN GP Loss: -50.26399612426758 G Loss: 32.97416687011719\n",
      "\n",
      "Epoch: 21 Iteration: 29440\n",
      "WGAN GP Loss: -44.99557876586914 G Loss: 30.311588287353516\n",
      "\n",
      "Epoch: 22 Iteration: 0\n",
      "WGAN GP Loss: -51.66925811767578 G Loss: 33.325618743896484\n",
      "\n",
      "Epoch: 22 Iteration: 1280\n",
      "WGAN GP Loss: -48.484535217285156 G Loss: 31.46207046508789\n",
      "\n",
      "Epoch: 22 Iteration: 2560\n",
      "WGAN GP Loss: -45.093929290771484 G Loss: 31.296329498291016\n",
      "\n",
      "Epoch: 22 Iteration: 3840\n",
      "WGAN GP Loss: -48.87767791748047 G Loss: 33.708709716796875\n",
      "\n",
      "Epoch: 22 Iteration: 5120\n",
      "WGAN GP Loss: -47.807769775390625 G Loss: 31.752405166625977\n",
      "\n",
      "Epoch: 22 Iteration: 6400\n",
      "WGAN GP Loss: -49.79389190673828 G Loss: 32.31082534790039\n",
      "\n",
      "Epoch: 22 Iteration: 7680\n",
      "WGAN GP Loss: -51.183135986328125 G Loss: 32.6390495300293\n",
      "\n",
      "Epoch: 22 Iteration: 8960\n",
      "WGAN GP Loss: -49.7665901184082 G Loss: 31.995086669921875\n",
      "\n",
      "Epoch: 22 Iteration: 10240\n",
      "WGAN GP Loss: -51.68585205078125 G Loss: 34.459938049316406\n",
      "\n",
      "Epoch: 22 Iteration: 11520\n",
      "WGAN GP Loss: -55.98284912109375 G Loss: 34.924224853515625\n",
      "\n",
      "Epoch: 22 Iteration: 12800\n",
      "WGAN GP Loss: -48.010231018066406 G Loss: 32.00592041015625\n",
      "\n",
      "Epoch: 22 Iteration: 14080\n",
      "WGAN GP Loss: -47.978607177734375 G Loss: 31.51718521118164\n",
      "\n",
      "Epoch: 22 Iteration: 15360\n",
      "WGAN GP Loss: -47.04364776611328 G Loss: 32.997047424316406\n",
      "\n",
      "Epoch: 22 Iteration: 16640\n",
      "WGAN GP Loss: -54.15396499633789 G Loss: 33.42621612548828\n",
      "\n",
      "Epoch: 22 Iteration: 17920\n",
      "WGAN GP Loss: -50.70105743408203 G Loss: 32.966087341308594\n",
      "\n",
      "Epoch: 22 Iteration: 19200\n",
      "WGAN GP Loss: -50.80866622924805 G Loss: 33.14040756225586\n",
      "\n",
      "Epoch: 22 Iteration: 20480\n",
      "WGAN GP Loss: -46.9538688659668 G Loss: 32.20032501220703\n",
      "\n",
      "Epoch: 22 Iteration: 21760\n",
      "WGAN GP Loss: -48.779300689697266 G Loss: 30.976318359375\n",
      "\n",
      "Epoch: 22 Iteration: 23040\n",
      "WGAN GP Loss: -45.657020568847656 G Loss: 30.96712875366211\n",
      "\n",
      "Epoch: 22 Iteration: 24320\n",
      "WGAN GP Loss: -48.806514739990234 G Loss: 32.56013107299805\n",
      "\n",
      "Epoch: 22 Iteration: 25600\n",
      "WGAN GP Loss: -48.17770004272461 G Loss: 31.93421745300293\n",
      "\n",
      "Epoch: 22 Iteration: 26880\n",
      "WGAN GP Loss: -44.14529037475586 G Loss: 29.795127868652344\n",
      "\n",
      "Epoch: 22 Iteration: 28160\n",
      "WGAN GP Loss: -45.59355545043945 G Loss: 30.557575225830078\n",
      "\n",
      "Epoch: 22 Iteration: 29440\n",
      "WGAN GP Loss: -47.97861862182617 G Loss: 33.624027252197266\n",
      "\n",
      "Epoch: 23 Iteration: 0\n",
      "WGAN GP Loss: -49.5126953125 G Loss: 31.045291900634766\n",
      "\n",
      "Epoch: 23 Iteration: 1280\n",
      "WGAN GP Loss: -41.12554168701172 G Loss: 30.866405487060547\n",
      "\n",
      "Epoch: 23 Iteration: 2560\n",
      "WGAN GP Loss: -45.85112762451172 G Loss: 33.84888458251953\n",
      "\n",
      "Epoch: 23 Iteration: 3840\n",
      "WGAN GP Loss: -46.780006408691406 G Loss: 32.336273193359375\n",
      "\n",
      "Epoch: 23 Iteration: 5120\n",
      "WGAN GP Loss: -53.2905158996582 G Loss: 33.08552169799805\n",
      "\n",
      "Epoch: 23 Iteration: 6400\n",
      "WGAN GP Loss: -48.75026321411133 G Loss: 31.961538314819336\n",
      "\n",
      "Epoch: 23 Iteration: 7680\n",
      "WGAN GP Loss: -43.22430419921875 G Loss: 31.73886489868164\n",
      "\n",
      "Epoch: 23 Iteration: 8960\n",
      "WGAN GP Loss: -44.08549118041992 G Loss: 31.272558212280273\n",
      "\n",
      "Epoch: 23 Iteration: 10240\n",
      "WGAN GP Loss: -43.60894012451172 G Loss: 32.505882263183594\n",
      "\n",
      "Epoch: 23 Iteration: 11520\n",
      "WGAN GP Loss: -47.158084869384766 G Loss: 33.38589859008789\n",
      "\n",
      "Epoch: 23 Iteration: 12800\n",
      "WGAN GP Loss: -47.067047119140625 G Loss: 29.616703033447266\n",
      "\n",
      "Epoch: 23 Iteration: 14080\n",
      "WGAN GP Loss: -51.15410614013672 G Loss: 32.822113037109375\n",
      "\n",
      "Epoch: 23 Iteration: 15360\n",
      "WGAN GP Loss: -51.1622428894043 G Loss: 33.46797180175781\n",
      "\n",
      "Epoch: 23 Iteration: 16640\n",
      "WGAN GP Loss: -45.29454803466797 G Loss: 32.31739807128906\n",
      "\n",
      "Epoch: 23 Iteration: 17920\n",
      "WGAN GP Loss: -49.85150146484375 G Loss: 32.97471237182617\n",
      "\n",
      "Epoch: 23 Iteration: 19200\n",
      "WGAN GP Loss: -46.94890594482422 G Loss: 30.7752742767334\n",
      "\n",
      "Epoch: 23 Iteration: 20480\n",
      "WGAN GP Loss: -46.916168212890625 G Loss: 33.43009567260742\n",
      "\n",
      "Epoch: 23 Iteration: 21760\n",
      "WGAN GP Loss: -45.32215118408203 G Loss: 32.74412536621094\n",
      "\n",
      "Epoch: 23 Iteration: 23040\n",
      "WGAN GP Loss: -53.2650260925293 G Loss: 33.675559997558594\n",
      "\n",
      "Epoch: 23 Iteration: 24320\n",
      "WGAN GP Loss: -46.48100280761719 G Loss: 31.526512145996094\n",
      "\n",
      "Epoch: 23 Iteration: 25600\n",
      "WGAN GP Loss: -43.00925064086914 G Loss: 30.685375213623047\n",
      "\n",
      "Epoch: 23 Iteration: 26880\n",
      "WGAN GP Loss: -51.82339859008789 G Loss: 33.233882904052734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 23 Iteration: 28160\n",
      "WGAN GP Loss: -46.143798828125 G Loss: 31.045108795166016\n",
      "\n",
      "Epoch: 23 Iteration: 29440\n",
      "WGAN GP Loss: -49.747249603271484 G Loss: 30.485034942626953\n",
      "\n",
      "Epoch: 24 Iteration: 0\n",
      "WGAN GP Loss: -48.998470306396484 G Loss: 32.019493103027344\n",
      "\n",
      "Epoch: 24 Iteration: 1280\n",
      "WGAN GP Loss: -48.301326751708984 G Loss: 31.479228973388672\n",
      "\n",
      "Epoch: 24 Iteration: 2560\n",
      "WGAN GP Loss: -39.9539680480957 G Loss: 28.777835845947266\n",
      "\n",
      "Epoch: 24 Iteration: 3840\n",
      "WGAN GP Loss: -45.33827209472656 G Loss: 31.178813934326172\n",
      "\n",
      "Epoch: 24 Iteration: 5120\n",
      "WGAN GP Loss: -48.408546447753906 G Loss: 31.112382888793945\n",
      "\n",
      "Epoch: 24 Iteration: 6400\n",
      "WGAN GP Loss: -49.90498352050781 G Loss: 30.955108642578125\n",
      "\n",
      "Epoch: 24 Iteration: 7680\n",
      "WGAN GP Loss: -49.46299362182617 G Loss: 33.647090911865234\n",
      "\n",
      "Epoch: 24 Iteration: 8960\n",
      "WGAN GP Loss: -56.04025650024414 G Loss: 34.290016174316406\n",
      "\n",
      "Epoch: 24 Iteration: 10240\n",
      "WGAN GP Loss: -37.65788269042969 G Loss: 29.58294677734375\n",
      "\n",
      "Epoch: 24 Iteration: 11520\n",
      "WGAN GP Loss: -49.17042541503906 G Loss: 32.13493347167969\n",
      "\n",
      "Epoch: 24 Iteration: 12800\n",
      "WGAN GP Loss: -48.26947784423828 G Loss: 29.51825714111328\n",
      "\n",
      "Epoch: 24 Iteration: 14080\n",
      "WGAN GP Loss: -46.876338958740234 G Loss: 31.959861755371094\n",
      "\n",
      "Epoch: 24 Iteration: 15360\n",
      "WGAN GP Loss: -46.32973861694336 G Loss: 33.934810638427734\n",
      "\n",
      "Epoch: 24 Iteration: 16640\n",
      "WGAN GP Loss: -46.298675537109375 G Loss: 31.48179054260254\n",
      "\n",
      "Epoch: 24 Iteration: 17920\n",
      "WGAN GP Loss: -55.16763687133789 G Loss: 33.965057373046875\n",
      "\n",
      "Epoch: 24 Iteration: 19200\n",
      "WGAN GP Loss: -48.477691650390625 G Loss: 31.44540023803711\n",
      "\n",
      "Epoch: 24 Iteration: 20480\n",
      "WGAN GP Loss: -44.276222229003906 G Loss: 30.939449310302734\n",
      "\n",
      "Epoch: 24 Iteration: 21760\n",
      "WGAN GP Loss: -45.776371002197266 G Loss: 31.839815139770508\n",
      "\n",
      "Epoch: 24 Iteration: 23040\n",
      "WGAN GP Loss: -50.23842239379883 G Loss: 34.735572814941406\n",
      "\n",
      "Epoch: 24 Iteration: 24320\n",
      "WGAN GP Loss: -50.380149841308594 G Loss: 32.597557067871094\n",
      "\n",
      "Epoch: 24 Iteration: 25600\n",
      "WGAN GP Loss: -48.00328826904297 G Loss: 32.88506317138672\n",
      "\n",
      "Epoch: 24 Iteration: 26880\n",
      "WGAN GP Loss: -47.517513275146484 G Loss: 32.07703399658203\n",
      "\n",
      "Epoch: 24 Iteration: 28160\n",
      "WGAN GP Loss: -48.375511169433594 G Loss: 31.53693389892578\n",
      "\n",
      "Epoch: 24 Iteration: 29440\n",
      "WGAN GP Loss: -46.323638916015625 G Loss: 32.2687873840332\n",
      "\n",
      "Epoch: 25 Iteration: 0\n",
      "WGAN GP Loss: -44.04512405395508 G Loss: 32.29420471191406\n",
      "\n",
      "Epoch: 25 Iteration: 1280\n",
      "WGAN GP Loss: -50.01691818237305 G Loss: 31.142911911010742\n",
      "\n",
      "Epoch: 25 Iteration: 2560\n",
      "WGAN GP Loss: -46.21812057495117 G Loss: 30.374507904052734\n",
      "\n",
      "Epoch: 25 Iteration: 3840\n",
      "WGAN GP Loss: -45.2676887512207 G Loss: 30.730453491210938\n",
      "\n",
      "Epoch: 25 Iteration: 5120\n",
      "WGAN GP Loss: -44.060340881347656 G Loss: 32.57502746582031\n",
      "\n",
      "Epoch: 25 Iteration: 6400\n",
      "WGAN GP Loss: -48.784297943115234 G Loss: 33.39197540283203\n",
      "\n",
      "Epoch: 25 Iteration: 7680\n",
      "WGAN GP Loss: -51.50874328613281 G Loss: 34.12871551513672\n",
      "\n",
      "Epoch: 25 Iteration: 8960\n",
      "WGAN GP Loss: -46.564781188964844 G Loss: 32.683380126953125\n",
      "\n",
      "Epoch: 25 Iteration: 10240\n",
      "WGAN GP Loss: -49.83176040649414 G Loss: 32.951637268066406\n",
      "\n",
      "Epoch: 25 Iteration: 11520\n",
      "WGAN GP Loss: -50.653175354003906 G Loss: 32.40108871459961\n",
      "\n",
      "Epoch: 25 Iteration: 12800\n",
      "WGAN GP Loss: -40.6309814453125 G Loss: 30.255847930908203\n",
      "\n",
      "Epoch: 25 Iteration: 14080\n",
      "WGAN GP Loss: -49.12276077270508 G Loss: 33.037010192871094\n",
      "\n",
      "Epoch: 25 Iteration: 15360\n",
      "WGAN GP Loss: -45.460205078125 G Loss: 31.60947608947754\n",
      "\n",
      "Epoch: 25 Iteration: 16640\n",
      "WGAN GP Loss: -49.6575927734375 G Loss: 32.98361587524414\n",
      "\n",
      "Epoch: 25 Iteration: 17920\n",
      "WGAN GP Loss: -47.67580032348633 G Loss: 34.29953384399414\n",
      "\n",
      "Epoch: 25 Iteration: 19200\n",
      "WGAN GP Loss: -53.186988830566406 G Loss: 34.05634689331055\n",
      "\n",
      "Epoch: 25 Iteration: 20480\n",
      "WGAN GP Loss: -46.6639289855957 G Loss: 30.927295684814453\n",
      "\n",
      "Epoch: 25 Iteration: 21760\n",
      "WGAN GP Loss: -50.75275421142578 G Loss: 34.79118347167969\n",
      "\n",
      "Epoch: 25 Iteration: 23040\n",
      "WGAN GP Loss: -48.5673713684082 G Loss: 34.201210021972656\n",
      "\n",
      "Epoch: 25 Iteration: 24320\n",
      "WGAN GP Loss: -46.849796295166016 G Loss: 35.183876037597656\n",
      "\n",
      "Epoch: 25 Iteration: 25600\n",
      "WGAN GP Loss: -47.39414978027344 G Loss: 34.052207946777344\n",
      "\n",
      "Epoch: 25 Iteration: 26880\n",
      "WGAN GP Loss: -47.975372314453125 G Loss: 31.751605987548828\n",
      "\n",
      "Epoch: 25 Iteration: 28160\n",
      "WGAN GP Loss: -48.48371887207031 G Loss: 30.156015396118164\n",
      "\n",
      "Epoch: 25 Iteration: 29440\n",
      "WGAN GP Loss: -47.36918258666992 G Loss: 31.16305160522461\n",
      "\n",
      "Epoch: 26 Iteration: 0\n",
      "WGAN GP Loss: -45.99531936645508 G Loss: 29.244272232055664\n",
      "\n",
      "Epoch: 26 Iteration: 1280\n",
      "WGAN GP Loss: -52.31620788574219 G Loss: 32.41869354248047\n",
      "\n",
      "Epoch: 26 Iteration: 2560\n",
      "WGAN GP Loss: -52.225399017333984 G Loss: 34.28990173339844\n",
      "\n",
      "Epoch: 26 Iteration: 3840\n",
      "WGAN GP Loss: -44.19770431518555 G Loss: 30.090267181396484\n",
      "\n",
      "Epoch: 26 Iteration: 5120\n",
      "WGAN GP Loss: -48.70201110839844 G Loss: 31.974594116210938\n",
      "\n",
      "Epoch: 26 Iteration: 6400\n",
      "WGAN GP Loss: -41.034873962402344 G Loss: 29.89657974243164\n",
      "\n",
      "Epoch: 26 Iteration: 7680\n",
      "WGAN GP Loss: -46.01588439941406 G Loss: 32.32007598876953\n",
      "\n",
      "Epoch: 26 Iteration: 8960\n",
      "WGAN GP Loss: -55.00143814086914 G Loss: 32.57225799560547\n",
      "\n",
      "Epoch: 26 Iteration: 10240\n",
      "WGAN GP Loss: -47.39664077758789 G Loss: 30.099225997924805\n",
      "\n",
      "Epoch: 26 Iteration: 11520\n",
      "WGAN GP Loss: -46.07941436767578 G Loss: 30.665019989013672\n",
      "\n",
      "Epoch: 26 Iteration: 12800\n",
      "WGAN GP Loss: -44.75114440917969 G Loss: 28.331775665283203\n",
      "\n",
      "Epoch: 26 Iteration: 14080\n",
      "WGAN GP Loss: -51.34328842163086 G Loss: 31.593982696533203\n",
      "\n",
      "Epoch: 26 Iteration: 15360\n",
      "WGAN GP Loss: -45.67634582519531 G Loss: 30.79257583618164\n",
      "\n",
      "Epoch: 26 Iteration: 16640\n",
      "WGAN GP Loss: -47.76344299316406 G Loss: 32.1578483581543\n",
      "\n",
      "Epoch: 26 Iteration: 17920\n",
      "WGAN GP Loss: -47.281227111816406 G Loss: 31.65361785888672\n",
      "\n",
      "Epoch: 26 Iteration: 19200\n",
      "WGAN GP Loss: -48.47379684448242 G Loss: 32.097110748291016\n",
      "\n",
      "Epoch: 26 Iteration: 20480\n",
      "WGAN GP Loss: -53.06096267700195 G Loss: 33.66432189941406\n",
      "\n",
      "Epoch: 26 Iteration: 21760\n",
      "WGAN GP Loss: -44.949188232421875 G Loss: 29.76273536682129\n",
      "\n",
      "Epoch: 26 Iteration: 23040\n",
      "WGAN GP Loss: -48.73195266723633 G Loss: 31.703750610351562\n",
      "\n",
      "Epoch: 26 Iteration: 24320\n",
      "WGAN GP Loss: -46.49834442138672 G Loss: 30.350997924804688\n",
      "\n",
      "Epoch: 26 Iteration: 25600\n",
      "WGAN GP Loss: -43.66481018066406 G Loss: 30.719572067260742\n",
      "\n",
      "Epoch: 26 Iteration: 26880\n",
      "WGAN GP Loss: -41.80118179321289 G Loss: 30.888813018798828\n",
      "\n",
      "Epoch: 26 Iteration: 28160\n",
      "WGAN GP Loss: -53.62013626098633 G Loss: 32.29581832885742\n",
      "\n",
      "Epoch: 26 Iteration: 29440\n",
      "WGAN GP Loss: -52.11276626586914 G Loss: 34.08497619628906\n",
      "\n",
      "Epoch: 27 Iteration: 0\n",
      "WGAN GP Loss: -41.35245895385742 G Loss: 30.019397735595703\n",
      "\n",
      "Epoch: 27 Iteration: 1280\n",
      "WGAN GP Loss: -48.53861618041992 G Loss: 32.29700469970703\n",
      "\n",
      "Epoch: 27 Iteration: 2560\n",
      "WGAN GP Loss: -50.29811477661133 G Loss: 33.886192321777344\n",
      "\n",
      "Epoch: 27 Iteration: 3840\n",
      "WGAN GP Loss: -49.9476432800293 G Loss: 32.4075813293457\n",
      "\n",
      "Epoch: 27 Iteration: 5120\n",
      "WGAN GP Loss: -48.47059631347656 G Loss: 32.907989501953125\n",
      "\n",
      "Epoch: 27 Iteration: 6400\n",
      "WGAN GP Loss: -52.85442352294922 G Loss: 32.543907165527344\n",
      "\n",
      "Epoch: 27 Iteration: 7680\n",
      "WGAN GP Loss: -49.91123962402344 G Loss: 31.837100982666016\n",
      "\n",
      "Epoch: 27 Iteration: 8960\n",
      "WGAN GP Loss: -52.0876350402832 G Loss: 30.75096321105957\n",
      "\n",
      "Epoch: 27 Iteration: 10240\n",
      "WGAN GP Loss: -51.7022705078125 G Loss: 33.502052307128906\n",
      "\n",
      "Epoch: 27 Iteration: 11520\n",
      "WGAN GP Loss: -46.646080017089844 G Loss: 31.78388023376465\n",
      "\n",
      "Epoch: 27 Iteration: 12800\n",
      "WGAN GP Loss: -51.5008430480957 G Loss: 31.728757858276367\n",
      "\n",
      "Epoch: 27 Iteration: 14080\n",
      "WGAN GP Loss: -48.852577209472656 G Loss: 31.543203353881836\n",
      "\n",
      "Epoch: 27 Iteration: 15360\n",
      "WGAN GP Loss: -50.33121109008789 G Loss: 32.31550598144531\n",
      "\n",
      "Epoch: 27 Iteration: 16640\n",
      "WGAN GP Loss: -48.359291076660156 G Loss: 31.688608169555664\n",
      "\n",
      "Epoch: 27 Iteration: 17920\n",
      "WGAN GP Loss: -49.1015510559082 G Loss: 32.449981689453125\n",
      "\n",
      "Epoch: 27 Iteration: 19200\n",
      "WGAN GP Loss: -38.791324615478516 G Loss: 28.78814125061035\n",
      "\n",
      "Epoch: 27 Iteration: 20480\n",
      "WGAN GP Loss: -47.45234680175781 G Loss: 32.978515625\n",
      "\n",
      "Epoch: 27 Iteration: 21760\n",
      "WGAN GP Loss: -46.32748031616211 G Loss: 30.94352912902832\n",
      "\n",
      "Epoch: 27 Iteration: 23040\n",
      "WGAN GP Loss: -46.86800003051758 G Loss: 31.68212127685547\n",
      "\n",
      "Epoch: 27 Iteration: 24320\n",
      "WGAN GP Loss: -52.14877700805664 G Loss: 32.912940979003906\n",
      "\n",
      "Epoch: 27 Iteration: 25600\n",
      "WGAN GP Loss: -48.98352813720703 G Loss: 32.1781005859375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 27 Iteration: 26880\n",
      "WGAN GP Loss: -45.458740234375 G Loss: 31.090065002441406\n",
      "\n",
      "Epoch: 27 Iteration: 28160\n",
      "WGAN GP Loss: -51.060508728027344 G Loss: 31.530933380126953\n",
      "\n",
      "Epoch: 27 Iteration: 29440\n",
      "WGAN GP Loss: -50.762359619140625 G Loss: 31.35879135131836\n",
      "\n",
      "Epoch: 28 Iteration: 0\n",
      "WGAN GP Loss: -46.355247497558594 G Loss: 30.70623207092285\n",
      "\n",
      "Epoch: 28 Iteration: 1280\n",
      "WGAN GP Loss: -51.753700256347656 G Loss: 32.61414337158203\n",
      "\n",
      "Epoch: 28 Iteration: 2560\n",
      "WGAN GP Loss: -51.11876678466797 G Loss: 32.1957893371582\n",
      "\n",
      "Epoch: 28 Iteration: 3840\n",
      "WGAN GP Loss: -45.93735122680664 G Loss: 30.888996124267578\n",
      "\n",
      "Epoch: 28 Iteration: 5120\n",
      "WGAN GP Loss: -49.08203125 G Loss: 30.480613708496094\n",
      "\n",
      "Epoch: 28 Iteration: 6400\n",
      "WGAN GP Loss: -51.029022216796875 G Loss: 33.43902587890625\n",
      "\n",
      "Epoch: 28 Iteration: 7680\n",
      "WGAN GP Loss: -49.607723236083984 G Loss: 32.908546447753906\n",
      "\n",
      "Epoch: 28 Iteration: 8960\n",
      "WGAN GP Loss: -44.238189697265625 G Loss: 28.953598022460938\n",
      "\n",
      "Epoch: 28 Iteration: 10240\n",
      "WGAN GP Loss: -50.28755569458008 G Loss: 32.120338439941406\n",
      "\n",
      "Epoch: 28 Iteration: 11520\n",
      "WGAN GP Loss: -52.588661193847656 G Loss: 32.50377655029297\n",
      "\n",
      "Epoch: 28 Iteration: 12800\n",
      "WGAN GP Loss: -47.33394241333008 G Loss: 31.592613220214844\n",
      "\n",
      "Epoch: 28 Iteration: 14080\n",
      "WGAN GP Loss: -44.06840896606445 G Loss: 28.53582191467285\n",
      "\n",
      "Epoch: 28 Iteration: 15360\n",
      "WGAN GP Loss: -47.554351806640625 G Loss: 29.373708724975586\n",
      "\n",
      "Epoch: 28 Iteration: 16640\n",
      "WGAN GP Loss: -47.96783447265625 G Loss: 31.294599533081055\n",
      "\n",
      "Epoch: 28 Iteration: 17920\n",
      "WGAN GP Loss: -51.880245208740234 G Loss: 33.18994140625\n",
      "\n",
      "Epoch: 28 Iteration: 19200\n",
      "WGAN GP Loss: -44.99683380126953 G Loss: 30.710308074951172\n",
      "\n",
      "Epoch: 28 Iteration: 20480\n",
      "WGAN GP Loss: -49.62687301635742 G Loss: 32.83318328857422\n",
      "\n",
      "Epoch: 28 Iteration: 21760\n",
      "WGAN GP Loss: -47.42509841918945 G Loss: 31.613765716552734\n",
      "\n",
      "Epoch: 28 Iteration: 23040\n",
      "WGAN GP Loss: -48.695335388183594 G Loss: 31.285541534423828\n",
      "\n",
      "Epoch: 28 Iteration: 24320\n",
      "WGAN GP Loss: -47.44962692260742 G Loss: 34.72935485839844\n",
      "\n",
      "Epoch: 28 Iteration: 25600\n",
      "WGAN GP Loss: -48.771820068359375 G Loss: 33.95581817626953\n",
      "\n",
      "Epoch: 28 Iteration: 26880\n",
      "WGAN GP Loss: -49.4221305847168 G Loss: 31.400522232055664\n",
      "\n",
      "Epoch: 28 Iteration: 28160\n",
      "WGAN GP Loss: -45.45341491699219 G Loss: 31.176555633544922\n",
      "\n",
      "Epoch: 28 Iteration: 29440\n",
      "WGAN GP Loss: -51.883567810058594 G Loss: 30.329669952392578\n",
      "\n",
      "Epoch: 29 Iteration: 0\n",
      "WGAN GP Loss: -45.566795349121094 G Loss: 31.017004013061523\n",
      "\n",
      "Epoch: 29 Iteration: 1280\n",
      "WGAN GP Loss: -43.08048629760742 G Loss: 31.796064376831055\n",
      "\n",
      "Epoch: 29 Iteration: 2560\n",
      "WGAN GP Loss: -46.566287994384766 G Loss: 30.20930290222168\n",
      "\n",
      "Epoch: 29 Iteration: 3840\n",
      "WGAN GP Loss: -50.628849029541016 G Loss: 32.32975769042969\n",
      "\n",
      "Epoch: 29 Iteration: 5120\n",
      "WGAN GP Loss: -48.17106628417969 G Loss: 32.579742431640625\n",
      "\n",
      "Epoch: 29 Iteration: 6400\n",
      "WGAN GP Loss: -48.755027770996094 G Loss: 32.146915435791016\n",
      "\n",
      "Epoch: 29 Iteration: 7680\n",
      "WGAN GP Loss: -47.15901184082031 G Loss: 32.067718505859375\n",
      "\n",
      "Epoch: 29 Iteration: 8960\n",
      "WGAN GP Loss: -46.14174270629883 G Loss: 31.835895538330078\n",
      "\n",
      "Epoch: 29 Iteration: 10240\n",
      "WGAN GP Loss: -48.92818832397461 G Loss: 31.84488868713379\n",
      "\n",
      "Epoch: 29 Iteration: 11520\n",
      "WGAN GP Loss: -43.28951644897461 G Loss: 29.61266326904297\n",
      "\n",
      "Epoch: 29 Iteration: 12800\n",
      "WGAN GP Loss: -47.76976013183594 G Loss: 31.11810874938965\n",
      "\n",
      "Epoch: 29 Iteration: 14080\n",
      "WGAN GP Loss: -49.920711517333984 G Loss: 31.961246490478516\n",
      "\n",
      "Epoch: 29 Iteration: 15360\n",
      "WGAN GP Loss: -48.96683120727539 G Loss: 31.07412338256836\n",
      "\n",
      "Epoch: 29 Iteration: 16640\n",
      "WGAN GP Loss: -55.06221008300781 G Loss: 34.23631286621094\n",
      "\n",
      "Epoch: 29 Iteration: 17920\n",
      "WGAN GP Loss: -47.614540100097656 G Loss: 29.89937973022461\n",
      "\n",
      "Epoch: 29 Iteration: 19200\n",
      "WGAN GP Loss: -44.54534149169922 G Loss: 30.482961654663086\n",
      "\n",
      "Epoch: 29 Iteration: 20480\n",
      "WGAN GP Loss: -48.6495246887207 G Loss: 32.48248291015625\n",
      "\n",
      "Epoch: 29 Iteration: 21760\n",
      "WGAN GP Loss: -51.18745422363281 G Loss: 32.0498161315918\n",
      "\n",
      "Epoch: 29 Iteration: 23040\n",
      "WGAN GP Loss: -48.499420166015625 G Loss: 31.203372955322266\n",
      "\n",
      "Epoch: 29 Iteration: 24320\n",
      "WGAN GP Loss: -43.41407012939453 G Loss: 31.576446533203125\n",
      "\n",
      "Epoch: 29 Iteration: 25600\n",
      "WGAN GP Loss: -52.65008544921875 G Loss: 31.118305206298828\n",
      "\n",
      "Epoch: 29 Iteration: 26880\n",
      "WGAN GP Loss: -49.16227340698242 G Loss: 31.2746524810791\n",
      "\n",
      "Epoch: 29 Iteration: 28160\n",
      "WGAN GP Loss: -49.444435119628906 G Loss: 32.535560607910156\n",
      "\n",
      "Epoch: 29 Iteration: 29440\n",
      "WGAN GP Loss: -49.49034118652344 G Loss: 31.331756591796875\n",
      "\n",
      "Epoch: 30 Iteration: 0\n",
      "WGAN GP Loss: -51.271244049072266 G Loss: 33.17394256591797\n",
      "\n",
      "Epoch: 30 Iteration: 1280\n",
      "WGAN GP Loss: -47.48728942871094 G Loss: 32.470741271972656\n",
      "\n",
      "Epoch: 30 Iteration: 2560\n",
      "WGAN GP Loss: -50.7507209777832 G Loss: 34.752227783203125\n",
      "\n",
      "Epoch: 30 Iteration: 3840\n",
      "WGAN GP Loss: -50.42542266845703 G Loss: 30.34697151184082\n",
      "\n",
      "Epoch: 30 Iteration: 5120\n",
      "WGAN GP Loss: -49.61722183227539 G Loss: 32.56599044799805\n",
      "\n",
      "Epoch: 30 Iteration: 6400\n",
      "WGAN GP Loss: -54.6043586730957 G Loss: 31.608787536621094\n",
      "\n",
      "Epoch: 30 Iteration: 7680\n",
      "WGAN GP Loss: -43.788455963134766 G Loss: 29.628034591674805\n",
      "\n",
      "Epoch: 30 Iteration: 8960\n",
      "WGAN GP Loss: -50.83515930175781 G Loss: 33.20383071899414\n",
      "\n",
      "Epoch: 30 Iteration: 10240\n",
      "WGAN GP Loss: -47.496543884277344 G Loss: 32.24881362915039\n",
      "\n",
      "Epoch: 30 Iteration: 11520\n",
      "WGAN GP Loss: -49.3717155456543 G Loss: 33.39824295043945\n",
      "\n",
      "Epoch: 30 Iteration: 12800\n",
      "WGAN GP Loss: -48.60276412963867 G Loss: 31.076904296875\n",
      "\n",
      "Epoch: 30 Iteration: 14080\n",
      "WGAN GP Loss: -45.71354675292969 G Loss: 31.021724700927734\n",
      "\n",
      "Epoch: 30 Iteration: 15360\n",
      "WGAN GP Loss: -50.479957580566406 G Loss: 33.51408386230469\n",
      "\n",
      "Epoch: 30 Iteration: 16640\n",
      "WGAN GP Loss: -53.2651481628418 G Loss: 32.539894104003906\n",
      "\n",
      "Epoch: 30 Iteration: 17920\n",
      "WGAN GP Loss: -49.94938659667969 G Loss: 32.06981658935547\n",
      "\n",
      "Epoch: 30 Iteration: 19200\n",
      "WGAN GP Loss: -45.3808479309082 G Loss: 31.724597930908203\n",
      "\n",
      "Epoch: 30 Iteration: 20480\n",
      "WGAN GP Loss: -52.17238998413086 G Loss: 32.2049560546875\n",
      "\n",
      "Epoch: 30 Iteration: 21760\n",
      "WGAN GP Loss: -48.87114715576172 G Loss: 31.69254493713379\n",
      "\n",
      "Epoch: 30 Iteration: 23040\n",
      "WGAN GP Loss: -49.50730895996094 G Loss: 29.842296600341797\n",
      "\n",
      "Epoch: 30 Iteration: 24320\n",
      "WGAN GP Loss: -50.45485305786133 G Loss: 32.778533935546875\n",
      "\n",
      "Epoch: 30 Iteration: 25600\n",
      "WGAN GP Loss: -48.40455627441406 G Loss: 32.01667785644531\n",
      "\n",
      "Epoch: 30 Iteration: 26880\n",
      "WGAN GP Loss: -44.79716491699219 G Loss: 29.8974666595459\n",
      "\n",
      "Epoch: 30 Iteration: 28160\n",
      "WGAN GP Loss: -48.909053802490234 G Loss: 32.431114196777344\n",
      "\n",
      "Epoch: 30 Iteration: 29440\n",
      "WGAN GP Loss: -51.717308044433594 G Loss: 30.68902587890625\n",
      "\n",
      "Epoch: 31 Iteration: 0\n",
      "WGAN GP Loss: -48.14744567871094 G Loss: 33.46813201904297\n",
      "\n",
      "Epoch: 31 Iteration: 1280\n",
      "WGAN GP Loss: -41.46620178222656 G Loss: 30.65367889404297\n",
      "\n",
      "Epoch: 31 Iteration: 2560\n",
      "WGAN GP Loss: -46.165409088134766 G Loss: 31.364498138427734\n",
      "\n",
      "Epoch: 31 Iteration: 3840\n",
      "WGAN GP Loss: -47.113651275634766 G Loss: 30.66134262084961\n",
      "\n",
      "Epoch: 31 Iteration: 5120\n",
      "WGAN GP Loss: -47.70995330810547 G Loss: 32.394229888916016\n",
      "\n",
      "Epoch: 31 Iteration: 6400\n",
      "WGAN GP Loss: -48.818851470947266 G Loss: 30.67108154296875\n",
      "\n",
      "Epoch: 31 Iteration: 7680\n",
      "WGAN GP Loss: -48.68421173095703 G Loss: 30.814193725585938\n",
      "\n",
      "Epoch: 31 Iteration: 8960\n",
      "WGAN GP Loss: -52.137027740478516 G Loss: 31.47644805908203\n",
      "\n",
      "Epoch: 31 Iteration: 10240\n",
      "WGAN GP Loss: -45.24357604980469 G Loss: 32.49991226196289\n",
      "\n",
      "Epoch: 31 Iteration: 11520\n",
      "WGAN GP Loss: -46.63839340209961 G Loss: 30.724349975585938\n",
      "\n",
      "Epoch: 31 Iteration: 12800\n",
      "WGAN GP Loss: -49.06013870239258 G Loss: 31.928394317626953\n",
      "\n",
      "Epoch: 31 Iteration: 14080\n",
      "WGAN GP Loss: -49.09358596801758 G Loss: 30.943479537963867\n",
      "\n",
      "Epoch: 31 Iteration: 15360\n",
      "WGAN GP Loss: -45.181800842285156 G Loss: 30.231687545776367\n",
      "\n",
      "Epoch: 31 Iteration: 16640\n",
      "WGAN GP Loss: -52.15765380859375 G Loss: 33.15655517578125\n",
      "\n",
      "Epoch: 31 Iteration: 17920\n",
      "WGAN GP Loss: -42.383140563964844 G Loss: 30.246379852294922\n",
      "\n",
      "Epoch: 31 Iteration: 19200\n",
      "WGAN GP Loss: -45.65353012084961 G Loss: 29.5517578125\n",
      "\n",
      "Epoch: 31 Iteration: 20480\n",
      "WGAN GP Loss: -44.605201721191406 G Loss: 30.570743560791016\n",
      "\n",
      "Epoch: 31 Iteration: 21760\n",
      "WGAN GP Loss: -46.85586166381836 G Loss: 30.199687957763672\n",
      "\n",
      "Epoch: 31 Iteration: 23040\n",
      "WGAN GP Loss: -54.358375549316406 G Loss: 35.11682891845703\n",
      "\n",
      "Epoch: 31 Iteration: 24320\n",
      "WGAN GP Loss: -48.43291473388672 G Loss: 32.06793975830078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 31 Iteration: 25600\n",
      "WGAN GP Loss: -47.39619445800781 G Loss: 31.235864639282227\n",
      "\n",
      "Epoch: 31 Iteration: 26880\n",
      "WGAN GP Loss: -51.98931121826172 G Loss: 31.70515251159668\n",
      "\n",
      "Epoch: 31 Iteration: 28160\n",
      "WGAN GP Loss: -50.23931121826172 G Loss: 32.18425750732422\n",
      "\n",
      "Epoch: 31 Iteration: 29440\n",
      "WGAN GP Loss: -45.153099060058594 G Loss: 30.859525680541992\n",
      "\n",
      "Epoch: 32 Iteration: 0\n",
      "WGAN GP Loss: -40.92937469482422 G Loss: 28.45077896118164\n",
      "\n",
      "Epoch: 32 Iteration: 1280\n",
      "WGAN GP Loss: -42.98257064819336 G Loss: 30.40206527709961\n",
      "\n",
      "Epoch: 32 Iteration: 2560\n",
      "WGAN GP Loss: -52.77229309082031 G Loss: 32.931549072265625\n",
      "\n",
      "Epoch: 32 Iteration: 3840\n",
      "WGAN GP Loss: -44.19158935546875 G Loss: 29.376750946044922\n",
      "\n",
      "Epoch: 32 Iteration: 5120\n",
      "WGAN GP Loss: -48.148193359375 G Loss: 29.79088020324707\n",
      "\n",
      "Epoch: 32 Iteration: 6400\n",
      "WGAN GP Loss: -51.046546936035156 G Loss: 32.48418426513672\n",
      "\n",
      "Epoch: 32 Iteration: 7680\n",
      "WGAN GP Loss: -49.992454528808594 G Loss: 31.72456169128418\n",
      "\n",
      "Epoch: 32 Iteration: 8960\n",
      "WGAN GP Loss: -48.37678909301758 G Loss: 31.260723114013672\n",
      "\n",
      "Epoch: 32 Iteration: 10240\n",
      "WGAN GP Loss: -47.74201965332031 G Loss: 30.7445125579834\n",
      "\n",
      "Epoch: 32 Iteration: 11520\n",
      "WGAN GP Loss: -47.94043731689453 G Loss: 29.22557258605957\n",
      "\n",
      "Epoch: 32 Iteration: 12800\n",
      "WGAN GP Loss: -46.733917236328125 G Loss: 30.67980194091797\n",
      "\n",
      "Epoch: 32 Iteration: 14080\n",
      "WGAN GP Loss: -50.56141662597656 G Loss: 33.92605972290039\n",
      "\n",
      "Epoch: 32 Iteration: 15360\n",
      "WGAN GP Loss: -44.277278900146484 G Loss: 30.63237190246582\n",
      "\n",
      "Epoch: 32 Iteration: 16640\n",
      "WGAN GP Loss: -47.43741989135742 G Loss: 31.66146469116211\n",
      "\n",
      "Epoch: 32 Iteration: 17920\n",
      "WGAN GP Loss: -47.625892639160156 G Loss: 32.70164489746094\n",
      "\n",
      "Epoch: 32 Iteration: 19200\n",
      "WGAN GP Loss: -50.933502197265625 G Loss: 32.6974983215332\n",
      "\n",
      "Epoch: 32 Iteration: 20480\n",
      "WGAN GP Loss: -51.53377151489258 G Loss: 32.43626403808594\n",
      "\n",
      "Epoch: 32 Iteration: 21760\n",
      "WGAN GP Loss: -43.01901626586914 G Loss: 27.38541603088379\n",
      "\n",
      "Epoch: 32 Iteration: 23040\n",
      "WGAN GP Loss: -51.46175765991211 G Loss: 33.392822265625\n",
      "\n",
      "Epoch: 32 Iteration: 24320\n",
      "WGAN GP Loss: -49.14035415649414 G Loss: 30.988662719726562\n",
      "\n",
      "Epoch: 32 Iteration: 25600\n",
      "WGAN GP Loss: -51.59718322753906 G Loss: 31.713481903076172\n",
      "\n",
      "Epoch: 32 Iteration: 26880\n",
      "WGAN GP Loss: -50.74896240234375 G Loss: 32.23728942871094\n",
      "\n",
      "Epoch: 32 Iteration: 28160\n",
      "WGAN GP Loss: -52.508148193359375 G Loss: 32.514915466308594\n",
      "\n",
      "Epoch: 32 Iteration: 29440\n",
      "WGAN GP Loss: -42.67108917236328 G Loss: 28.822078704833984\n",
      "\n",
      "Epoch: 33 Iteration: 0\n",
      "WGAN GP Loss: -46.01585388183594 G Loss: 30.66314697265625\n",
      "\n",
      "Epoch: 33 Iteration: 1280\n",
      "WGAN GP Loss: -51.6247444152832 G Loss: 32.13081741333008\n",
      "\n",
      "Epoch: 33 Iteration: 2560\n",
      "WGAN GP Loss: -47.51559829711914 G Loss: 32.78062438964844\n",
      "\n",
      "Epoch: 33 Iteration: 3840\n",
      "WGAN GP Loss: -45.34003829956055 G Loss: 30.496103286743164\n",
      "\n",
      "Epoch: 33 Iteration: 5120\n",
      "WGAN GP Loss: -48.70363235473633 G Loss: 31.56891632080078\n",
      "\n",
      "Epoch: 33 Iteration: 6400\n",
      "WGAN GP Loss: -49.10302734375 G Loss: 30.94939422607422\n",
      "\n",
      "Epoch: 33 Iteration: 7680\n",
      "WGAN GP Loss: -50.72086715698242 G Loss: 31.223831176757812\n",
      "\n",
      "Epoch: 33 Iteration: 8960\n",
      "WGAN GP Loss: -46.24360656738281 G Loss: 32.55542755126953\n",
      "\n",
      "Epoch: 33 Iteration: 10240\n",
      "WGAN GP Loss: -46.226905822753906 G Loss: 33.54339599609375\n",
      "\n",
      "Epoch: 33 Iteration: 11520\n",
      "WGAN GP Loss: -48.4569206237793 G Loss: 30.29759979248047\n",
      "\n",
      "Epoch: 33 Iteration: 12800\n",
      "WGAN GP Loss: -47.847652435302734 G Loss: 29.998167037963867\n",
      "\n",
      "Epoch: 33 Iteration: 14080\n",
      "WGAN GP Loss: -45.44070816040039 G Loss: 28.747051239013672\n",
      "\n",
      "Epoch: 33 Iteration: 15360\n",
      "WGAN GP Loss: -49.94124984741211 G Loss: 32.613643646240234\n",
      "\n",
      "Epoch: 33 Iteration: 16640\n",
      "WGAN GP Loss: -53.93612289428711 G Loss: 33.49504470825195\n",
      "\n",
      "Epoch: 33 Iteration: 17920\n",
      "WGAN GP Loss: -49.21199035644531 G Loss: 31.798336029052734\n",
      "\n",
      "Epoch: 33 Iteration: 19200\n",
      "WGAN GP Loss: -54.11668014526367 G Loss: 32.80440139770508\n",
      "\n",
      "Epoch: 33 Iteration: 20480\n",
      "WGAN GP Loss: -50.76052474975586 G Loss: 33.33271789550781\n",
      "\n",
      "Epoch: 33 Iteration: 21760\n",
      "WGAN GP Loss: -50.28068923950195 G Loss: 31.486804962158203\n",
      "\n",
      "Epoch: 33 Iteration: 23040\n",
      "WGAN GP Loss: -49.80683517456055 G Loss: 30.923852920532227\n",
      "\n",
      "Epoch: 33 Iteration: 24320\n",
      "WGAN GP Loss: -45.9248161315918 G Loss: 31.491626739501953\n",
      "\n",
      "Epoch: 33 Iteration: 25600\n",
      "WGAN GP Loss: -46.64676284790039 G Loss: 29.697864532470703\n",
      "\n",
      "Epoch: 33 Iteration: 26880\n",
      "WGAN GP Loss: -47.31999206542969 G Loss: 29.85251235961914\n",
      "\n",
      "Epoch: 33 Iteration: 28160\n",
      "WGAN GP Loss: -51.088653564453125 G Loss: 33.246177673339844\n",
      "\n",
      "Epoch: 33 Iteration: 29440\n",
      "WGAN GP Loss: -47.618412017822266 G Loss: 30.21429443359375\n",
      "\n",
      "Epoch: 34 Iteration: 0\n",
      "WGAN GP Loss: -54.03413772583008 G Loss: 31.97124481201172\n",
      "\n",
      "Epoch: 34 Iteration: 1280\n",
      "WGAN GP Loss: -48.977874755859375 G Loss: 31.528032302856445\n",
      "\n",
      "Epoch: 34 Iteration: 2560\n",
      "WGAN GP Loss: -51.18600845336914 G Loss: 31.009445190429688\n",
      "\n",
      "Epoch: 34 Iteration: 3840\n",
      "WGAN GP Loss: -47.04690933227539 G Loss: 31.638280868530273\n",
      "\n",
      "Epoch: 34 Iteration: 5120\n",
      "WGAN GP Loss: -48.03331756591797 G Loss: 32.210697174072266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Start\")\n",
    "# For each epoch\n",
    "\n",
    "for epoch in range(epo, num_epochs):\n",
    "    count = 0\n",
    "    itera = 0\n",
    "    # For each batch in the dataloader\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        \n",
    "        netD.zero_grad()\n",
    "        \n",
    "        real_imgs = data[0].to(device)\n",
    "\n",
    "        \n",
    "        mini_batch = real_imgs.size(0)\n",
    "\n",
    "        loss_real = netD(real_imgs)\n",
    "        loss_real = -torch.mean(loss_real)\n",
    "        loss_real.backward()\n",
    "\n",
    "        noise = torch.randn(mini_batch, nz, 1, 1, device=device)\n",
    "\n",
    "        fake_imgs = netG(noise)\n",
    "\n",
    "        loss_fake = netD(fake_imgs.detach())\n",
    "        loss_fake = torch.mean(loss_fake)\n",
    "        loss_fake.backward()\n",
    "\n",
    "        alpha = torch.randn(mini_batch, 1, 1, 1, device = device)\n",
    "\n",
    "        interp = alpha * real_imgs + ((1-alpha) * fake_imgs.detach())\n",
    "        interp.requires_grad_()\n",
    "\n",
    "        model_interp = netD(interp)\n",
    "\n",
    "        grads = torch.autograd.grad(outputs=model_interp, inputs=interp,\n",
    "                                  grad_outputs=torch.ones(model_interp.size()).to(device),\n",
    "                                  create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "\n",
    "        grads = torch.square(grads)\n",
    "        grads = torch.sum(grads, dim = [1,2,3])\n",
    "        grads = torch.sqrt(grads)\n",
    "        grads = grads - 1\n",
    "        grads = torch.square(grads)\n",
    "        grad_pen = torch.mean(grads * lambda_gp)\n",
    "\n",
    "        grad_pen.backward()\n",
    "\n",
    "\n",
    "        d_loss = loss_fake - loss_real + grad_pen\n",
    "\n",
    "        optimizerD.step()\n",
    "#         print(output_real)\n",
    "\n",
    "#             print(itera)\n",
    "        if i % d_ratio == 0:\n",
    "\n",
    "            netG.zero_grad()\n",
    "\n",
    "            g_loss = netD(fake_imgs)\n",
    "\n",
    "            g_loss = -torch.mean(g_loss)\n",
    "            g_loss.backward()\n",
    "            optimizerG.step()\n",
    "\n",
    "            if i % 10 == 0:\n",
    "\n",
    "                print(\"\\nEpoch:\", epoch, \"Iteration:\", count)\n",
    "                print(\"WGAN GP Loss:\", d_loss.item(), \"G Loss:\", g_loss.item())\n",
    "\n",
    "\n",
    "            # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        if (i % 10 == 0):\n",
    "            with torch.no_grad():\n",
    "                    guess = netG(fixed_noise)\n",
    "                    guess = guess.cpu()\n",
    "                    old_min = -1\n",
    "                    old_max = 1\n",
    "                    old_range = old_max - old_min\n",
    "                    new_range = 1 - 0\n",
    "                    guess = (((guess - old_min)*new_range)/ old_range) + 0\n",
    "                    guess = guess.permute(0,2,3,1)\n",
    "\n",
    "                    fig = plt.figure(figsize=(4,4))\n",
    "                    for i in range(16):\n",
    "                        plt.subplot(4, 4, i+1)\n",
    "                        plt.imshow(guess[i, :, :])\n",
    "\n",
    "                        plt.axis('off')\n",
    "                    path = \"./Training_Imgs/Epoch: \" + str(epoch) + \" training_step: \" + str(count) + \".png\"\n",
    "                    plt.savefig(path, dpi=300)\n",
    "                    plt.close('all')\n",
    "        count += mini_batch\n",
    "\n",
    "    path = \"./Saved_Models/WGAN_CheckPoint_Epoch:_\" + str(epoch)\n",
    "    torch.save({\n",
    "    'netD_state_dict': netD.state_dict(),\n",
    "    'netG_state_dict': netG.state_dict(),\n",
    "    'optimizerD_state_dict': optimizerD.state_dict(),\n",
    "    'optimizerG_state_dict': optimizerG.state_dict(),\n",
    "    'epoch': epoch\n",
    "    }, path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
